{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import deeppavlov\n",
    "from deeppavlov.dataset_readers.ontonotes_reader import OntonotesReader\n",
    "from deeppavlov.models.preprocessors.capitalization import CapitalizationPreprocessor\n",
    "from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeTagMainPart(tag:str):\n",
    "    return tag[2:] if tag != 'O' else tag\n",
    "\n",
    "def tags2binary(tags, symb=True, types:list=None):\n",
    "    tags = copy.deepcopy(tags)\n",
    "    for seq in tags:\n",
    "        for i in range(len(seq)):\n",
    "            ne = False\n",
    "            if seq[i] != 'O':\n",
    "                ne = True\n",
    "            if types and (getNeTagMainPart(seq[i]) not in types):\n",
    "                ne = False\n",
    "            if symb:\n",
    "                seq[i] = 'T' if ne else 'O'\n",
    "            else:\n",
    "                seq[i] = 1 if ne else 0\n",
    "#     print(np.sum(np.array([1 if t == 1 else 0 for seq in tags for t in seq])))\n",
    "    return tags\n",
    "\n",
    "def to_lower_case(tokens:list):\n",
    "    tokens_lower = []\n",
    "    for seq in tokens:\n",
    "        tokens_lower.append([])\n",
    "        for token in seq:\n",
    "            tokens_lower[-1].append(token.lower())\n",
    "    return tokens_lower\n",
    "\n",
    "def get_tokens_len(tokens):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    return [len(seq) for seq in tokens]\n",
    "\n",
    "def add_padding(tokens:list):\n",
    "    if isinstance(tokens[0], str):\n",
    "        return tokens, len(tokens)\n",
    "    elif isinstance(tokens[0], list):\n",
    "        tokens = copy.deepcopy(tokens)\n",
    "        max_len = 0\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_len:\n",
    "                max_len = len(seq)\n",
    "        for seq in tokens:\n",
    "            i = len(seq)\n",
    "            while i < max_len:\n",
    "                seq.append('')\n",
    "                i += 1\n",
    "        return tokens\n",
    "    else:\n",
    "        raise Exception('tokens should be either list of strings or list of lists of strings')\n",
    "        \n",
    "def flatten_list(ar:list):\n",
    "    flat = []\n",
    "    for sublist in ar:\n",
    "        flat += sublist\n",
    "    return flat\n",
    "\n",
    "def select_list_elements(ar:list, indices:list):\n",
    "    return [ar[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'valid', 'test'])\n",
      "Num of train sentences: 75187\n",
      "Num of valid sentences: 9603\n",
      "Num of test sentences: 9479\n",
      "[(['Actions', 'had', 'to', 'be', 'taken', 'to', 'break', 'through', 'the', 'blockade', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'a', 'night', 'in', 'late', 'July', '1940', ',', 'the', 'atmosphere', 'in', 'Zhuanbi', 'Village', 'in', 'Shaanxi', 'was', 'unusual', '.'], ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'B-GPE', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'that', 'dark', 'night', ',', 'everyone', 'was', 'sleeping', 'when', 'human', 'voices', 'and', 'neighing', 'horses', 'were', 'heard', 'within', 'the', 'village', '.'], ['O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['People', 'all', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Did', 'something', 'happen', '?'], ['O', 'O', 'O', 'O']), (['Some', 'folks', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Opening', 'the', 'street', 'gate', ',', 'they', 'saw', 'a', 'soldier', 'standing', 'by', 'the', 'gate', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Folks', ',', 'go', 'back', ',', 'go', 'back', ',', 'nothing', 'is', 'wrong', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Our', 'troops', 'are', 'just', 'going', 'to', 'stay', 'here', 'for', 'the', 'night', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "reader = OntonotesReader()\n",
    "dataset = reader.read(data_path='data/')\n",
    "print(dataset.keys())\n",
    "print('Num of train sentences: {}'.format(len(dataset['train'])))\n",
    "print('Num of valid sentences: {}'.format(len(dataset['valid'])))\n",
    "print('Num of test sentences: {}'.format(len(dataset['test'])))\n",
    "print(dataset['train'][50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select sentences with chosen NE types only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_ne_type(data:list, ne_types:list, tags2binary=False):\n",
    "    if ne_types == None or len(ne_types) == 0:\n",
    "        return data\n",
    "    data_filtered = []\n",
    "    for tokens,tags in data:\n",
    "        contains_all = True\n",
    "        tags_norm = [getNeTagMainPart(t) for t in tags]\n",
    "        for ne_type in ne_types:\n",
    "            if not ne_type in tags_norm:\n",
    "                contains_all = False\n",
    "                break\n",
    "        if contains_all:\n",
    "            if tags2binary:\n",
    "                tags = ['T' if t in ne_types else 'O' for t in tags_norm]\n",
    "            data_filtered.append((tokens,tags))\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train sentences: 12195\n",
      "Num of valid sentences: 1553\n",
      "Num of test sentences: 1573\n"
     ]
    }
   ],
   "source": [
    "NE_TYPES = ['PERSON']\n",
    "for dataset_type in ['train', 'valid', 'test']:\n",
    "    dataset[dataset_type] = filter_data_by_ne_type(dataset[dataset_type], NE_TYPES, tags2binary=True)\n",
    "    print('Num of {} sentences: {}'.format(dataset_type, len(dataset[dataset_type])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select few examples as support set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Terry', 'L.', 'Haines', ',', 'formerly', 'general', 'manager', 'of', 'Canadian', 'operations', ',', 'was', 'elected', 'to', 'the', 'new', 'position', 'of', 'vice', 'president', ',', 'North', 'American', 'sales', ',', 'of', 'this', 'plastics', 'concern', '.'], ['T', 'T', 'T', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['-LRB-', 'photo', 'by', 'Pu', 'Hua', '-', 'chih', '-RRB-'], ['O', 'O', 'O', 'T', 'T', 'T', 'T', 'O']), (['Teng', 'had', 'originally', 'set', 'himself', 'a', 'goal', 'of', 'getting', 'his', 'tenth', 'book', 'written', 'and', 'published', 'last', 'year', '.'], ['T', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['CNN', \"'s\", 'David', 'Mattingly', 'joins', 'us', 'with', 'the', 'latest', 'on', 'the', 'investigation', '/.'], ['O', 'O', 'T', 'T', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['-LRB-', 'courtesy', 'of', 'Yeh', 'Chin', '-', 'tai', '-RRB-'], ['O', 'O', 'O', 'T', 'T', 'T', 'T', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "n_examples = 5\n",
    "# np.random.seed(12)\n",
    "indices = np.random.choice(len(dataset['train']), size=n_examples)\n",
    "examples = [dataset['train'][i] for i in indices]\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split tokens and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_tags(dataset: list):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for sample in dataset:\n",
    "        tokens.append(sample[0])\n",
    "        tags.append(sample[1])\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train,tags_train = split_tokens_tags(examples)\n",
    "# print(tags_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_data_props(tokens:list, tags:list):\n",
    "    props = {}\n",
    "    props['ne_types'] = {}\n",
    "    tokens_flat = flatten_list(tokens)\n",
    "    tags_flat = flatten_list(tags)\n",
    "    ne_count = 0\n",
    "    for tag in tags_flat:\n",
    "        if tag != 'O':\n",
    "            ne_count += 1\n",
    "            tag_main = tag[2:]\n",
    "            if props['ne_types'].get(tag_main) != None:\n",
    "                props['ne_types'][tag_main] += 1\n",
    "            else:\n",
    "                props['ne_types'][tag_main] = 1\n",
    "    props['sent_count'] = len(tokens)            \n",
    "    props['tokens_count'] = len(tokens_flat)\n",
    "    props['ne_count'] = ne_count\n",
    "    props['ne_ratio'] = props['ne_count']/props['tokens_count']\n",
    "    for k in props['ne_types'].keys():   \n",
    "        props['ne_types'][k] /= ne_count\n",
    "        \n",
    "    return props\n",
    "        \n",
    "def print_data_props(props:dict):\n",
    "    s = ''\n",
    "    s += '#sentences = {}, '.format(props['sent_count'])\n",
    "    s += '#tokens = {}, '.format(props['tokens_count'])\n",
    "    s += '#ne = {}, '.format(props['ne_count'])\n",
    "    s += '#ne / #tokens = {:.3f}, '.format(props['ne_ratio'])\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences = 5, #tokens = 77, #ne = 14, #ne / #tokens = 0.182, \n"
     ]
    }
   ],
   "source": [
    "print_data_props(calc_data_props(tokens_train,tags_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elmo wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoEmbedder():\n",
    "    def __init__(self):\n",
    "        self.elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.sess = sess\n",
    "\n",
    "    def get_tokens_embeddings(self, tokens_input: list, tokens_length:list=None):\n",
    "        if not tokens_length:\n",
    "            if isinstance(tokens_input[0], list):\n",
    "                tokens_length = [len(seq) for seq in tokens_input]\n",
    "            else:\n",
    "                tokens_length = len(tokens_input)\n",
    "        embeddings = self.elmo(\n",
    "                        inputs={\n",
    "                            \"tokens\": tokens_input,\n",
    "                            \"sequence_len\": tokens_length\n",
    "                        },\n",
    "                        signature=\"tokens\",\n",
    "                        as_dict=True)[\"elmo\"]\n",
    "        embeddings = self.sess.run([embeddings])\n",
    "        return embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeEmbedder():\n",
    "    def __init__(self, use_elmo=True, elmo_scale=1., use_cap_feat=False, use_glove=False):\n",
    "        self.use_elmo = use_elmo\n",
    "        self.elmo_scale = elmo_scale\n",
    "        self.use_cap_feat = use_cap_feat\n",
    "        self.use_glove = use_glove\n",
    "        if self.use_elmo:\n",
    "            self.elmo = ElmoEmbedder()\n",
    "        if self.use_cap_feat:\n",
    "            self.cap_prep = CapitalizationPreprocessor()\n",
    "        if self.use_glove:\n",
    "            self.glove = GloVeEmbedder('embeddings/glove.6B/glove.6B.100d.txt', pad_zero=True)\n",
    "        \n",
    "    def embed(self, tokens: list):\n",
    "        if isinstance(tokens[0], str):\n",
    "            tokens = [tokens]\n",
    "        # Get ELMo embeddings\n",
    "        if self.use_elmo:\n",
    "            tokens_input = add_padding(tokens)\n",
    "            tokens_length = get_tokens_len(tokens)\n",
    "            embeddings = self.elmo.get_tokens_embeddings(tokens_input, tokens_length)\n",
    "            embeddings *= self.elmo_scale\n",
    "            embed_size = embeddings.shape[-1]\n",
    "#             print(embeddings.shape)\n",
    "#             print(embed_size)\n",
    "\n",
    "        # Use capitalization features\n",
    "        if self.use_cap_feat:\n",
    "#             print('Use capitalization features')\n",
    "            cap_features = self.cap_prep(tokens)\n",
    "    #         print(cap_features)\n",
    "#             print(cap_features.shape)\n",
    "            embeddings = np.concatenate((embeddings, cap_features), axis=2)\n",
    "            embed_size = embeddings.shape[-1]\n",
    "#             print(embeddings.shape)\n",
    "\n",
    "        # Use GloVe embeddings\n",
    "        if self.use_glove:\n",
    "#             print('Use GloVe')\n",
    "            \n",
    "            glove_embed = self.glove(to_lower_case(tokens))\n",
    "            glove_embed = np.array(glove_embed)\n",
    "            if not self.use_elmo:\n",
    "                embeddings = glove_embed\n",
    "            else: \n",
    "                embeddings = np.concatenate((embeddings, glove_embed), axis=2)\n",
    "            embed_size = embeddings.shape[-1]\n",
    "#             print(embeddings.shape)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity of token embedding vector to some prototype (centroid) or just support vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sim(token_vec, support_vec)->dict:\n",
    "    sim = {}\n",
    "    sim['euc_dist'] = np.linalg.norm(token_vec - support_vec)\n",
    "    sim['dot_prod'] = np.dot(token_vec, support_vec)\n",
    "    sim['cosine'] = np.dot(token_vec, support_vec)/(np.linalg.norm(token_vec)*np.linalg.norm(support_vec)) if np.linalg.norm(support_vec) != 0 else 0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sim_batch(tokens: list, embeddings: np.ndarray, support_vec: np.ndarray)->list:\n",
    "    sim_list = []\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    for i in range(len(tokens_length)):\n",
    "        sim_list.append([])\n",
    "        for j in range(tokens_length[i]):\n",
    "            token_vec = embeddings[i,j,:]\n",
    "            sim_list[i].append(calc_sim(token_vec, support_vec))\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sim(sim_list):\n",
    "    sims_flat = {'euc_dist': [], 'dot_prod': [], 'cosine': []}\n",
    "    for i in range(len(sim_list)):\n",
    "        for j in range(len(sim_list[i])):\n",
    "            for sim_type in ['euc_dist', 'dot_prod', 'cosine']:\n",
    "                sims_flat[sim_type].append(sim_list[i][j][sim_type])\n",
    "    for sim_type in ['euc_dist', 'dot_prod', 'cosine']:\n",
    "        sims_flat[sim_type] = np.array(sims_flat[sim_type])\n",
    "    return sims_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate centroid for named entities embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ne_centroid_vec(tokens: list, tags: list, embeddings: np.ndarray=None, embedder: CompositeEmbedder=None):\n",
    "\n",
    "    # Calculate embeddings\n",
    "    if embedder != None:\n",
    "        embeddings = embedder.embed(tokens)\n",
    "\n",
    "    # Calculate average vector for ne-tags\n",
    "    embed_size = embeddings.shape[-1]\n",
    "    ne_prototype = np.zeros((embed_size,))\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    tags_bin = np.array(flatten_list(tags2binary(tags, symb=False)))\n",
    "#     print(tags_bin)\n",
    "    n_ne_tags = np.sum(tags_bin == 1)\n",
    "    embeddings_ne_flat = np.zeros((n_ne_tags, embed_size))\n",
    "#     print(n_ne_tags)\n",
    "#     n_ne_tags = 0\n",
    "    k = 0\n",
    "    for i in range(len(tokens_length)):\n",
    "        for j in range(tokens_length[i]):\n",
    "            if tags[i][j] == 'T' or tags[i][j] == 1:\n",
    "                ne_prototype += embeddings[i,j,:].reshape((embed_size,))\n",
    "                embeddings_ne_flat[k,:] = embeddings[i,j,:]\n",
    "                k += 1\n",
    "#                 n_ne_tags += 1\n",
    "    if n_ne_tags != 0:\n",
    "        ne_prototype /= n_ne_tags\n",
    "    print('ne mean vector: {}'.format(ne_prototype))\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sim_list = calc_sim_batch(tokens, embeddings, ne_prototype)\n",
    "\n",
    "    return ne_prototype, sim_list, embeddings, embeddings_ne_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarities of some test tokens to NE prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sim_to_ne_prototype(tokens: list, ne_prototype: np.ndarray, embeddings: np.ndarray=None, embedder: CompositeEmbedder=None):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    \n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    \n",
    "    # Calculate embeddings\n",
    "    if embedder != None:\n",
    "        embeddings = embedder.embed(tokens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sim_list = calc_sim_batch(tokens, embeddings, ne_prototype)\n",
    "    \n",
    "    return sim_list, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sim_to_ne_nearest(tokens: list, ne_support_embeddings: np.ndarray, embeddings: np.ndarray=None, embedder: CompositeEmbedder=None):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    \n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    \n",
    "    # Calculate embeddings\n",
    "    if embedder != None:\n",
    "        embeddings = embedder.embed(tokens)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    n_supports = ne_support_embeddings.shape[0]\n",
    "    sim_list = []\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    for i in range(len(tokens_length)):\n",
    "        sim_list.append([])\n",
    "        for j in range(tokens_length[i]):\n",
    "            token_vec = embeddings[i,j,:]\n",
    "            sim_token_list = {'euc_dist': [], 'dot_prod': [], 'cosine': []}\n",
    "            for k in range(n_supports):\n",
    "                sim = calc_sim(token_vec, ne_support_embeddings[k, :])\n",
    "                for sim_type in ['euc_dist', 'dot_prod', 'cosine']:\n",
    "                    sim_token_list[sim_type].append(sim[sim_type])\n",
    "            sim_list[i].append({'euc_dist': np.min(np.array(sim_token_list['euc_dist'])), \n",
    "                                'dot_prod': np.max(np.array(sim_token_list['dot_prod'])), \n",
    "                                'cosine': np.max(np.array(sim_token_list['cosine']))})\n",
    "            \n",
    "    return sim_list, embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/aggregation/scaling:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.36 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/aggregation/scaling:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/scaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/aggregation/weights:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.42 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/aggregation/weights:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.48 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.55 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.62 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.68 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.75 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.83 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/W_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.91 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/W_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.99 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.105 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.113 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.120 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.128 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.133 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN/b_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.142 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN/b_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_0/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_carry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.149 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_0/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_carry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_0/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.156 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_0/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_0/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_carry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.164 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_0/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_carry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_0/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.174 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_0/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_1/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_carry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.180 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_1/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_carry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_1/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.186 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_1/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_1/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_carry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.195 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_1/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_carry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_high_1/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.203 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_high_1/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_proj/W_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/W_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.214 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_proj/W_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/W_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/CNN_proj/b_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/b_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.221 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/CNN_proj/b_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/b_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.228 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.234 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.242 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.252 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.257 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.265 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.272 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.279 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.285 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.295 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.301 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.308 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_3/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/char_embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 18:33:54.315 INFO in 'tensorflow'['tf_logging'] at line 116: Initialize variable module_3/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/char_embed\n",
      "2018-08-09 18:33:55.529 INFO in 'deeppavlov.models.embedders.glove_embedder'['glove_embedder'] at line 60: [loading embeddings from `/home/kostrovskiy/prog/AI/NLP/DeepPavlov/download/embeddings/glove.6B/glove.6B.100d.txt`]\n",
      "2018-08-09 18:33:55.529 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 204: loading projection weights from /home/kostrovskiy/prog/AI/NLP/DeepPavlov/download/embeddings/glove.6B/glove.6B.100d.txt\n",
      "2018-08-09 18:33:55.530 DEBUG in 'smart_open.smart_open_lib'['smart_open_lib'] at line 176: {'kw': {}, 'mode': 'rb', 'uri': '/home/kostrovskiy/prog/AI/NLP/DeepPavlov/download/embeddings/glove.6B/glove.6B.100d.txt'}\n",
      "2018-08-09 18:34:24.945 INFO in 'gensim.models.keyedvectors'['keyedvectors'] at line 266: loaded (400000, 100) matrix from /home/kostrovskiy/prog/AI/NLP/DeepPavlov/download/embeddings/glove.6B/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "embedder = CompositeEmbedder(use_elmo=True, elmo_scale=1, use_cap_feat=True, use_glove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate NE centroid for support examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne mean vector: [-0.14085243 -0.24215232  0.18345754 ... -0.88435143 -0.03901565\n",
      " -0.46350143]\n",
      "(14, 1128)\n"
     ]
    }
   ],
   "source": [
    "ne_prototype, _, _, ne_support_embeddings = calc_ne_centroid_vec(tokens_train, tags_train, embedder=embedder)\n",
    "print(ne_support_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select some unlabeled examples from the test set and estimate similarity to a named entity for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences = 100, #tokens = 2269, #ne = 222, #ne / #tokens = 0.098, \n"
     ]
    }
   ],
   "source": [
    "n_test_sentences = 100\n",
    "# np.random.seed(44)\n",
    "indices_test = np.random.choice(len(dataset['test']), size=n_test_sentences)\n",
    "test_sentences = select_list_elements(dataset['test'], indices_test)\n",
    "tokens_test,tags_test = split_tokens_tags(test_sentences)\n",
    "print_data_props(calc_data_props(tokens_test,tags_test))\n",
    "# print(tokens_test)\n",
    "# print(tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list_test = {}\n",
    "sim_list_test['centroid'], _ = calc_sim_to_ne_prototype(tokens_test, ne_prototype, embedder=embedder)\n",
    "sim_list_test['nearest'], _ = calc_sim_to_ne_nearest(tokens_test, ne_support_embeddings, embedder=embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group similarities with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_tokens_sim(tokens: list, sim_list: list, sim_type='cosine'):\n",
    "    tokens_sim = []\n",
    "    for i in range(len(tokens)):\n",
    "        tokens_sim.append([])\n",
    "        for j in range(len(tokens[i])):\n",
    "            sim = sim_list[i][j][sim_type] if isinstance(sim_list[i][j], dict) else sim_list[i][j]\n",
    "            tokens_sim[-1].append((tokens[i][j], sim))\n",
    "    return tokens_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test_sim = {}\n",
    "tokens_test_sim['centroid'] = zip_tokens_sim(tokens_test, sim_list_test['centroid'])\n",
    "tokens_test_sim['nearest'] = zip_tokens_sim(tokens_test, sim_list_test['nearest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print test sentences with NE similarities estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate_ne_token(token, tag):\n",
    "    if tag == 'T':\n",
    "        token = '[[' + token + ']]'\n",
    "    return token\n",
    "def format_labeled_examples(tokens_input: list, tags_input: list):\n",
    "    s = '+++++++++ Input examples +++++++++\\n\\n'\n",
    "    for i in range(len(tokens_input)):\n",
    "        for j in range(len(tokens_input[i])):\n",
    "            s += decorate_ne_token(tokens_input[i][j], tags_input[i][j]) + ' '\n",
    "        s += '\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_inference_results(tokens_sim: list):\n",
    "    s = '+++++++++ Tests +++++++++\\n\\n'\n",
    "    for seq in tokens_sim:\n",
    "        for token, sim in seq:\n",
    "            s += '{}[{:.3f}]'.format(token, sim)\n",
    "            s += ' '\n",
    "        s += '\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "text += format_labeled_examples(tokens_train, tags_train)\n",
    "text += format_inference_results(tokens_test_sim['centroid'])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "text += format_labeled_examples(tokens_train, tags_train)\n",
    "text += format_inference_results(tokens_test_sim['nearest'])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize similarities of tokens to NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(red=0, green=255, blue=0):\n",
    "    return {'r': red, 'g': green, 'b': blue}\n",
    "\n",
    "def get_rgba_str(color, alpha=1):\n",
    "    return 'rgba({},{},{},{})'.format(color['r'], color['g'], color['b'], alpha)\n",
    "\n",
    "def get_token_span_str(token, color, cf=1):\n",
    "    return '<span style=\"padding: 0.15em; margin-right: 4px; border-radius: 0.25em; background: {};\">{}</span>'.format(get_rgba_str(color, alpha=cf), token)\n",
    "\n",
    "def wrap_with_style(html):\n",
    "    return '<div style=\"line-height: 1.5em;\">{:s}</div>'.format(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sim_min_max(sim_list):\n",
    "    sim_flat = flatten_sim(sim_list)\n",
    "    sim_min = np.min(sim_flat['cosine'])\n",
    "    sim_max = np.max(sim_flat['cosine'])\n",
    "    return (sim_min, sim_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_transform_lin(sim, sim_min, sim_max):\n",
    "    # similarity transformation for better visualization\n",
    "    return (sim - sim_min)/(sim_max - sim_min)\n",
    "def sim_transform(sim, sim_min, sim_max, T=0.5):\n",
    "    # similarity transformation with temperature for better visualization\n",
    "    return (np.exp(sim/T) - np.exp(sim_min/T))/(np.exp(sim_max/T) - np.exp(sim_min/T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colored_results_html(tokens_sim: list, sim_min_max:tuple, color, T=0.5, caption=None):\n",
    "    sim_min, sim_max = sim_min_max\n",
    "    if not caption:\n",
    "        caption = 'Visualization of tokens to NE similarities on test set'\n",
    "    s = '<h3 style=\"margin-bottom:0.3em;\">{}</h3>'.format(caption)\n",
    "    for seq in tokens_sim:\n",
    "        ch = 0\n",
    "        for token, sim in seq:\n",
    "            s += get_token_span_str(token, color, cf=sim_transform(sim, sim_min, sim_max, T))\n",
    "            ch += len(token)\n",
    "            if ch > 100:\n",
    "                s += '<br/>'\n",
    "                ch = 0\n",
    "#             s += ' '\n",
    "        s += '<br/><br/>'\n",
    "    return wrap_with_style(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list_bin_train = tags2binary(tags_train, symb=False)\n",
    "tokens_train_sim = zip_tokens_sim(tokens_train, tags_list_bin_train)\n",
    "display(HTML(get_colored_results_html(tokens_train_sim, (0,1), color=get_color(244,50,244), caption='Support examples')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = get_color()\n",
    "display(HTML(get_colored_results_html(tokens_test_sim['centroid'][:10], calc_sim_min_max(sim_list_test['centroid'][:10]), bg_color)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(get_colored_results_html(tokens_test_sim['nearest'][:10], calc_sim_min_max(sim_list_test['nearest'][:10]), bg_color)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histograms of test similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags_test_bin_flat = np.array(flatten_list(tags2binary(tags_test, symb=False))) # TODO: bag here\n",
    "# print(np.sum(np.array([1 if t == 1 else 0 for seq in tags2binary(tags_test, symb=False) for t in seq])))\n",
    "tags_test_bin_flat = np.array([1 if t == 'T' else 0 for seq in tags_test for t in seq])\n",
    "dif_tags = set()\n",
    "for seq in tags_test:\n",
    "    for t in seq:\n",
    "        dif_tags.add(t)\n",
    "print(dif_tags)\n",
    "print(tags_test_bin_flat.shape)\n",
    "print(np.sum(tags_test_bin_flat))\n",
    "sim_test_words = {}\n",
    "sim_test_ne = {}\n",
    "plt.figure(figsize=(12,5))\n",
    "for i, method in enumerate(['centroid', 'nearest']):\n",
    "    sim_flat = np.array(flatten_sim(sim_list_test[method])['cosine'])\n",
    "    sim_test_words[method] = sim_flat[tags_test_bin_flat == 0]\n",
    "    sim_test_ne[method] = sim_flat[tags_test_bin_flat == 1]\n",
    "    print(sim_test_words[method].shape)\n",
    "    print(sim_test_ne[method].shape)\n",
    "    plt.subplot(1, 2, i+1, title=method)\n",
    "    plt.hist(sim_test_words[method], normed=True, color = 'green', edgecolor = 'black',\n",
    "         bins = 30, label='words', fc=(0, 1, 0, 0.3))\n",
    "    plt.hist(sim_test_ne[method], normed=True, color = 'red', edgecolor = 'black',\n",
    "             bins = 30, label='named entities', fc=(1, 0, 0, 0.3))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot probability densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15415455/plotting-probability-density-function-by-sample-with-matplotlib\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace\n",
    "plt.figure(figsize=(12,5))\n",
    "for i, method in enumerate(['centroid', 'nearest']):\n",
    "    kde_words = gaussian_kde( sim_test_words[method] )\n",
    "    dist_space_words = linspace( min(sim_test_words[method]), max(sim_test_words[method]), 100 )\n",
    "    kde_ne = gaussian_kde( sim_test_ne[method] )\n",
    "    dist_space_ne = linspace( min(sim_test_ne[method]), max(sim_test_ne[method]), 100 )\n",
    "    plt.subplot(1, 2, i+1, title=method)\n",
    "    plt.plot( dist_space_words, kde_words(dist_space_words), color='green', label='words' )\n",
    "    plt.plot( dist_space_ne, kde_ne(dist_space_ne), color='red',  label='named entities' )\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "#     plt.title('Probability densities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_tags(sim_list, sim_type, T=0.5, threshold=0.5):\n",
    "    sim_min, sim_max = calc_sim_min_max(sim_list)\n",
    "    tokens_length = get_tokens_len(sim_list)\n",
    "    tags = [['T' if sim_transform(sim_list[i][j][sim_type], sim_min, sim_max, T)  > threshold else 'O' for j in range(tokens_length[i])] for i in range(len(tokens_length))]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = {}\n",
    "for i, method in enumerate(['centroid', 'nearest']):\n",
    "    sim_list = sim_list_test[method]  \n",
    "    tags_test_pred = infer_tags(sim_list, 'cosine', T=1, threshold=0.4)\n",
    "#     target = flatten_list(tags2binary(tags_test, symb=False))\n",
    "#     pred = flatten_list(tags2binary(tags_test_pred, symb=False))\n",
    "    target = [1 if t == 'T' else 0 for seq in tags_test for t in seq]\n",
    "    pred = [1 if t == 'T' else 0 for seq in tags_test_pred for t in seq]\n",
    "#     print(sum(target))\n",
    "#     print(sum(pred))\n",
    "    f1[method] = f1_score(target, pred)\n",
    "print('F1 scores:')\n",
    "for k in f1.keys():\n",
    "    print('{}: {:3f}'.format(k, f1[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('+++++ Samples stats +++++')\n",
    "print('Support set:')\n",
    "print_data_props(calc_data_props(tokens_train,tags_train))\n",
    "print('Tests:')\n",
    "print_data_props(calc_data_props(tokens_test,tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.array([1 if t == 'T' else 0 for seq in tags_test for t in seq])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
