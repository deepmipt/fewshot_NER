{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fewshot_ner_viz_component.data_processing import *\n",
    "from src.fewshot_ner_viz_component.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.layers.tf_layers import cudnn_bi_lstm, cudnn_bi_gru, bi_rnn, stacked_cnn, INITIALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ELMO = False\n",
    "TRAIN_ALL_ELMO_PARAMS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train sentences: 75187\n",
      "Num of valid sentences: 9603\n",
      "Num of test sentences: 9479\n",
      "[(['Actions', 'had', 'to', 'be', 'taken', 'to', 'break', 'through', 'the', 'blockade', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'a', 'night', 'in', 'late', 'July', '1940', ',', 'the', 'atmosphere', 'in', 'Zhuanbi', 'Village', 'in', 'Shaanxi', 'was', 'unusual', '.'], ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'B-GPE', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'that', 'dark', 'night', ',', 'everyone', 'was', 'sleeping', 'when', 'human', 'voices', 'and', 'neighing', 'horses', 'were', 'heard', 'within', 'the', 'village', '.'], ['O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['People', 'all', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Did', 'something', 'happen', '?'], ['O', 'O', 'O', 'O']), (['Some', 'folks', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Opening', 'the', 'street', 'gate', ',', 'they', 'saw', 'a', 'soldier', 'standing', 'by', 'the', 'gate', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Folks', ',', 'go', 'back', ',', 'go', 'back', ',', 'nothing', 'is', 'wrong', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Our', 'troops', 'are', 'just', 'going', 'to', 'stay', 'here', 'for', 'the', 'night', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n",
      "Num of train sentences: 12195\n",
      "Num of valid sentences: 1553\n",
      "Num of test sentences: 1573\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = read_data()\n",
    "ne_type = 'PERSON'\n",
    "dataset = filter_dataset_by_ne_types(dataset_orig, ne_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIALIZER = tf.contrib.layers.xavier_initializer\n",
    "def build_cudnn_rnn(units, mask, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        with tf.variable_scope(cell_type.upper() + '_' + str(n)):\n",
    "            if cell_type.lower() == 'lstm':\n",
    "                units, _ = cudnn_bi_lstm(units, n_hidden, sequence_lengths)\n",
    "            elif cell_type.lower() == 'gru':\n",
    "                units, _ = cudnn_bi_gru(units, n_hidden, sequence_lengths)\n",
    "            else:\n",
    "                raise RuntimeError('Wrong cell type \"{}\"! Only \"gru\" and \"lstm\"!'.format(cell_type))\n",
    "            units = tf.concat(units, -1)\n",
    "            if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "                units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_rnn(units, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        units, _ = bi_rnn(units, n_hidden, cell_type=cell_type, name='Layer_' + str(n))\n",
    "        units = tf.concat(units, -1)\n",
    "        if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "            units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_top(units, n_tags=1, top_dropout:bool=False, two_dense_on_top:bool=False, n_hidden=128):\n",
    "    if top_dropout:\n",
    "        units = variational_dropout(units, dropout_ph)\n",
    "    if two_dense_on_top:\n",
    "        units = tf.layers.dense(units, n_hidden, activation=tf.nn.relu,\n",
    "                                kernel_initializer=INITIALIZER(),\n",
    "                                kernel_regularizer=tf.nn.l2_loss)\n",
    "    logits = tf.layers.dense(units, n_tags, activation=None,\n",
    "                             kernel_initializer=INITIALIZER(),\n",
    "                             kernel_regularizer=tf.nn.l2_loss)\n",
    "    return logits\n",
    "\n",
    "def build_train_predict(logits, n_tags, mask, sequence_lengths, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg):\n",
    "    res = {}\n",
    "    if use_crf:\n",
    "        sequence_lengths = tf.reduce_sum(mask, axis=1)\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(logits, y_ph, sequence_lengths)\n",
    "        loss_tensor = -log_likelihood\n",
    "        res['transition_params'] = transition_params\n",
    "    else:\n",
    "        ground_truth_labels = tf.one_hot(y_ph, n_tags)\n",
    "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_labels, logits=logits)\n",
    "        loss_tensor = loss_tensor * mask\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        res['y_pred'] = y_pred\n",
    "\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "\n",
    "    # L2 regularization\n",
    "    if l2_reg > 0:\n",
    "        loss += l2_reg * tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    res['loss'] = loss\n",
    "        \n",
    "    # optimizer = partial(tf.train.MomentumOptimizer, momentum=0.9, use_nesterov=True)\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    train_op = get_train_op(loss, learning_rate_ph, optimizer, clip_norm=clip_grad_norm)\n",
    "    res['train_op'] = train_op\n",
    "    return res\n",
    "\n",
    "def predict_no_crf(y_pred, mask, feed_dict):\n",
    "    pred_idxs, mask = sess.run([y_pred, mask], feed_dict)\n",
    "\n",
    "    # Filter by sequece length\n",
    "    sequence_lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "    pred = []\n",
    "    for utt, l in zip(pred_idxs, sequence_lengths):\n",
    "        pred.append(utt[:l])\n",
    "    return pred\n",
    "\n",
    "def predict_crf(logits, transition_params, mask, feed_dict):\n",
    "    logits, trans_params, mask = sess.run([logits,\n",
    "                                           transition_params,\n",
    "                                           mask],\n",
    "                                           feed_dict=feed_dict)\n",
    "    sequence_lengths = np.maximum(np.sum(mask, axis=1).astype(np.int32), 1)\n",
    "    # iterate over the sentences because no batching in viterbi_decode\n",
    "    y_pred = []\n",
    "    for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "        logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
    "        viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "        y_pred += [viterbi_seq]\n",
    "    return y_pred\n",
    "\n",
    "def get_train_op(loss,\n",
    "                 learning_rate,\n",
    "                 optimizer=None,\n",
    "                 clip_norm=None,\n",
    "                 learnable_scopes=None,\n",
    "                 optimizer_scope_name=None):\n",
    "    \"\"\" Get train operation for given loss\n",
    "\n",
    "    Args:\n",
    "        loss: loss, tf tensor or scalar\n",
    "        learning_rate: scalar or placeholder\n",
    "        clip_norm: clip gradients norm by clip_norm\n",
    "        learnable_scopes: which scopes are trainable (None for all)\n",
    "        optimizer: instance of tf.train.Optimizer, default Adam\n",
    "\n",
    "    Returns:\n",
    "        train_op\n",
    "    \"\"\"\n",
    "    if optimizer_scope_name is None:\n",
    "        opt_scope = tf.variable_scope('Optimizer')\n",
    "    else:\n",
    "        opt_scope = tf.variable_scope(optimizer_scope_name)\n",
    "    with opt_scope:\n",
    "        if learnable_scopes is None:\n",
    "            variables_to_train = tf.global_variables()\n",
    "        else:\n",
    "            variables_to_train = []\n",
    "            for scope_name in learnable_scopes:\n",
    "                for var in tf.global_variables():\n",
    "                    if scope_name in var.name:\n",
    "                        variables_to_train.append(var)\n",
    "\n",
    "        if optimizer is None:\n",
    "            optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "        # For batch norm it is necessary to update running averages\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            opt = optimizer(learning_rate)\n",
    "            grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n",
    "            if clip_norm is not None:\n",
    "                grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                  for grad, var in grads_and_vars] #  if grad is not None\n",
    "            train_op = opt.apply_gradients(grads_and_vars)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(prob: np.ndarray, threshold=0.5):\n",
    "    labels = np.zeros(prob.shape)\n",
    "    labels[prob > threshold] = 1\n",
    "    return labels\n",
    "def flat_array(a: np.ndarray):\n",
    "#     return np.reshape(a, a.size)\n",
    "    return a.flatten()\n",
    "def calc_f1(y, pred_prob):\n",
    "    return f1_score(flat_array(y), flat_array(predict_labels(pred_prob)))\n",
    "def tags2binaryPadded(tags:list):\n",
    "    if isinstance(tags[0], str):\n",
    "        tags = [tags]\n",
    "    n_sentences = len(tags)\n",
    "    tokens_length = get_tokens_len(tags)\n",
    "    max_len = np.max(tokens_length)\n",
    "    tokens_length = np.tile(np.expand_dims(tokens_length, -1), (1,max_len))\n",
    "    y = np.zeros((n_sentences, max_len))\n",
    "    range_ar = np.tile(np.arange(1, max_len+1, 1), (n_sentences, 1))\n",
    "    for i, sen in enumerate(tags):\n",
    "        for j, tag in enumerate(sen):\n",
    "            if tags[i][j] != 'O':\n",
    "                y[i][j] = 1\n",
    "#     y[range_ar > tokens_length] = -1\n",
    "    return y\n",
    "def get_batch(dataset, batch_size=None):\n",
    "    if not batch_size:\n",
    "        batch_size = len(dataset)\n",
    "    tokens, tags = get_data_sample(dataset, batch_size)\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    tokens = add_padding(tokens)\n",
    "    y = tags2binaryPadded(tags)\n",
    "    return tokens, tags, tokens_length, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-24 19:05:58.998 INFO in 'tensorflow'['tf_logging'] at line 159: Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=TRAIN_ELMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.trainable_variables())\n",
    "if(TRAIN_ELMO):\n",
    "    elmo_params = {'layer_coefficients': tf.trainable_variables()[-2], 'scaling': tf.trainable_variables()[-1]}\n",
    "    print(elmo_params)\n",
    "elmo_vars = tf.trainable_variables()\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}\n",
    "if TRAIN_ALL_ELMO_PARAMS:\n",
    "    cell0_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0']\n",
    "    cell1_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "use_cudnn_rnn = False\n",
    "l2_reg = 0.01\n",
    "n_hidden_list = (128,)\n",
    "cell_type = 'lstm'\n",
    "n_tags = 2\n",
    "use_crf = True\n",
    "clip_grad_norm = 5.0\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "tokens_input_ph = tf.placeholder(shape=[None, None], dtype=tf.string)\n",
    "tokens_length_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "y_ph = tf.placeholder(shape=[None, None], dtype=tf.int32, name='y_ph')\n",
    "learning_rate_ph = tf.placeholder_with_default(learning_rate, shape=[], name='learning_rate')\n",
    "dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name='dropout')\n",
    "training_ph = tf.placeholder_with_default(False, shape=[], name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-24 19:05:59.854 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n",
      "/home/clement/virtenv/env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "emb = elmo(inputs={\"tokens\": tokens_input_ph,\n",
    "                    \"sequence_len\": tokens_length_ph},\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)['elmo']\n",
    "mask = tf.sequence_mask(lengths=tokens_length_ph, dtype=tf.float32)\n",
    "features = emb\n",
    "if use_cudnn_rnn:\n",
    "    units = build_cudnn_rnn(features, mask, n_hidden_list, cell_type)\n",
    "else:\n",
    "    units = build_rnn(features, n_hidden_list, cell_type)\n",
    "\n",
    "logits = build_top(units, n_tags=n_tags)\n",
    "\n",
    "out_dict = build_train_predict(logits, n_tags, mask, tokens_length_ph, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg)\n",
    "train_op = out_dict['train_op']\n",
    "loss = out_dict['loss']\n",
    "if use_crf:\n",
    "    transition_params = out_dict['transition_params']\n",
    "else:\n",
    "    y_pred = out_dict['y_pred']\n",
    "\n",
    "predict = predict_crf if use_crf else predict_no_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Layer_0_LSTM/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(1152, 512) dtype=float32_ref>, <tf.Variable 'Layer_0_LSTM/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'Layer_0_LSTM/bidirectional_rnn/bw/lstm_cell/kernel:0' shape=(1152, 512) dtype=float32_ref>, <tf.Variable 'Layer_0_LSTM/bidirectional_rnn/bw/lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(256, 2) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'transitions:0' shape=(2, 2) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "model_vars = [v for v in all_vars if v not in elmo_vars]\n",
    "print(model_vars)\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run([initialize_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sen_size = 100\n",
    "tokens_valid, tags_valid, tokens_len_valid, y_valid = get_batch(dataset['valid'], valid_sen_size)\n",
    "feed_valid = {tokens_input_ph: tokens_valid, tokens_length_ph: tokens_len_valid, y_ph: y_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/200\n",
      "Train loss = 9.197015762329102\n",
      "Valid loss = 6.311370372772217\n",
      "Step 2/200\n",
      "Step 3/200\n",
      "Step 4/200\n",
      "Step 5/200\n",
      "Train loss = 4.33183479309082\n",
      "Step 6/200\n",
      "Step 7/200\n",
      "Step 8/200\n",
      "Step 9/200\n",
      "Step 10/200\n",
      "Train loss = 1.7397642135620117\n",
      "Valid loss = 1.5812721252441406\n",
      "Step 11/200\n",
      "Step 12/200\n",
      "Step 13/200\n",
      "Step 14/200\n",
      "Step 15/200\n",
      "Train loss = 1.2727947235107422\n",
      "Step 16/200\n",
      "Step 17/200\n",
      "Step 18/200\n",
      "Step 19/200\n",
      "Step 20/200\n",
      "Train loss = 0.4056224822998047\n",
      "Valid loss = 1.8678202629089355\n",
      "Step 21/200\n",
      "Step 22/200\n",
      "Step 23/200\n",
      "Step 24/200\n",
      "Step 25/200\n",
      "Train loss = 2.3028202056884766\n",
      "Step 26/200\n",
      "Step 27/200\n",
      "Step 28/200\n",
      "Step 29/200\n",
      "Step 30/200\n",
      "Train loss = 0.41156673431396484\n",
      "Valid loss = 1.4713494777679443\n",
      "Step 31/200\n",
      "Step 32/200\n",
      "Step 33/200\n",
      "Step 34/200\n",
      "Step 35/200\n",
      "Train loss = 0.9195282459259033\n",
      "Step 36/200\n",
      "Step 37/200\n",
      "Step 38/200\n",
      "Step 39/200\n",
      "Step 40/200\n",
      "Train loss = 1.0717930793762207\n",
      "Valid loss = 1.463246464729309\n",
      "Step 41/200\n",
      "Step 42/200\n",
      "Step 43/200\n",
      "Step 44/200\n",
      "Step 45/200\n",
      "Train loss = 2.0763401985168457\n",
      "Step 46/200\n",
      "Step 47/200\n",
      "Step 48/200\n",
      "Step 49/200\n",
      "Step 50/200\n",
      "Train loss = 0.8017520904541016\n",
      "Valid loss = 1.5233474969863892\n",
      "Step 51/200\n",
      "Step 52/200\n",
      "Step 53/200\n",
      "Step 54/200\n",
      "Step 55/200\n",
      "Train loss = 2.8548121452331543\n",
      "Step 56/200\n",
      "Step 57/200\n",
      "Step 58/200\n",
      "Step 59/200\n",
      "Step 60/200\n",
      "Train loss = 2.8611092567443848\n",
      "Valid loss = 1.126476764678955\n",
      "Step 61/200\n",
      "Step 62/200\n",
      "Step 63/200\n",
      "Step 64/200\n",
      "Step 65/200\n",
      "Train loss = 1.5478167533874512\n",
      "Step 66/200\n",
      "Step 67/200\n",
      "Step 68/200\n",
      "Step 69/200\n",
      "Step 70/200\n",
      "Train loss = 0.44345521926879883\n",
      "Valid loss = 1.1971087455749512\n",
      "Step 71/200\n",
      "Step 72/200\n",
      "Step 73/200\n",
      "Step 74/200\n",
      "Step 75/200\n",
      "Train loss = 0.7864031791687012\n",
      "Step 76/200\n",
      "Step 77/200\n",
      "Step 78/200\n",
      "Step 79/200\n",
      "Step 80/200\n",
      "Train loss = 0.33397865295410156\n",
      "Valid loss = 1.155443787574768\n",
      "Step 81/200\n",
      "Step 82/200\n",
      "Step 83/200\n",
      "Step 84/200\n",
      "Step 85/200\n",
      "Train loss = 0.0808866024017334\n",
      "Step 86/200\n",
      "Step 87/200\n",
      "Step 88/200\n",
      "Step 89/200\n",
      "Step 90/200\n",
      "Train loss = 0.3150653839111328\n",
      "Valid loss = 1.3495888710021973\n",
      "Step 91/200\n",
      "Step 92/200\n",
      "Step 93/200\n",
      "Step 94/200\n",
      "Step 95/200\n",
      "Train loss = 0.8734219074249268\n",
      "Step 96/200\n",
      "Step 97/200\n",
      "Step 98/200\n",
      "Step 99/200\n",
      "Step 100/200\n",
      "Train loss = 0.545039176940918\n",
      "Valid loss = 1.5153038501739502\n",
      "Step 101/200\n",
      "Step 102/200\n",
      "Step 103/200\n",
      "Step 104/200\n",
      "Step 105/200\n",
      "Train loss = 0.7219085693359375\n",
      "Step 106/200\n",
      "Step 107/200\n",
      "Step 108/200\n",
      "Step 109/200\n",
      "Step 110/200\n",
      "Train loss = 3.15494441986084\n",
      "Valid loss = 1.0457054376602173\n",
      "Step 111/200\n",
      "Step 112/200\n",
      "Step 113/200\n",
      "Step 114/200\n",
      "Step 115/200\n",
      "Train loss = 7.287434101104736\n",
      "Step 116/200\n",
      "Step 117/200\n",
      "Step 118/200\n",
      "Step 119/200\n",
      "Step 120/200\n",
      "Train loss = 1.0800979137420654\n",
      "Valid loss = 1.1865674257278442\n",
      "Step 121/200\n",
      "Step 122/200\n",
      "Step 123/200\n",
      "Step 124/200\n",
      "Step 125/200\n",
      "Train loss = 0.849217414855957\n",
      "Step 126/200\n",
      "Step 127/200\n",
      "Step 128/200\n",
      "Step 129/200\n",
      "Step 130/200\n",
      "Train loss = 0.8285198211669922\n",
      "Valid loss = 1.4564526081085205\n",
      "Step 131/200\n",
      "Step 132/200\n",
      "Step 133/200\n",
      "Step 134/200\n",
      "Step 135/200\n",
      "Train loss = 0.5212597846984863\n",
      "Step 136/200\n",
      "Step 137/200\n",
      "Step 138/200\n",
      "Step 139/200\n",
      "Step 140/200\n",
      "Train loss = 1.0798406600952148\n",
      "Valid loss = 1.1508809328079224\n",
      "Step 141/200\n",
      "Step 142/200\n",
      "Step 143/200\n",
      "Step 144/200\n",
      "Step 145/200\n",
      "Train loss = 0.2983260154724121\n",
      "Step 146/200\n",
      "Step 147/200\n",
      "Step 148/200\n",
      "Step 149/200\n",
      "Step 150/200\n",
      "Train loss = 0.20101451873779297\n",
      "Valid loss = 1.1999415159225464\n",
      "Step 151/200\n",
      "Step 152/200\n",
      "Step 153/200\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200\n",
    "batch_size = 64\n",
    "display_step = 5\n",
    "valid_step = 10\n",
    "losses = {'train': [], 'valid': []}\n",
    "f1_scores = {'train': [], 'valid': []}\n",
    "best_valid_f1 = 0\n",
    "d_elmo_cells_list = {'cell0':[], 'cell1':[]}\n",
    "for step in range(1, num_steps+1):\n",
    "    print('Step {}/{}'.format(step, num_steps))\n",
    "    tokens_batch, tags_batch, tokens_len_batch, y_batch = get_batch(dataset['train'], batch_size)\n",
    "    feed = {tokens_input_ph: tokens_batch, tokens_length_ph: tokens_len_batch, y_ph: y_batch}\n",
    "    if TRAIN_ALL_ELMO_PARAMS:\n",
    "        cell0_kernel_val1 = cell0_kernel.eval(session=sess)\n",
    "        cell1_kernel_val1 = cell1_kernel.eval(session=sess)\n",
    "    # Train\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        loss_cur, _ = sess.run([loss, train_op], feed_dict=feed)\n",
    "    losses['train'].append(loss_cur)\n",
    "    if TRAIN_ALL_ELMO_PARAMS:\n",
    "        cell0_kernel_val2 = cell0_kernel.eval(session=sess)\n",
    "        cell1_kernel_val2 = cell1_kernel.eval(session=sess)\n",
    "        d_cell0_kernel = np.linalg.norm(cell0_kernel_val2 - cell0_kernel_val1)/np.linalg.norm(cell0_kernel_val1)\n",
    "        d_cell1_kernel = np.linalg.norm(cell1_kernel_val2 - cell1_kernel_val1)/np.linalg.norm(cell1_kernel_val1)\n",
    "        d_elmo_cells_list['cell0'].append(d_cell0_kernel)\n",
    "        d_elmo_cells_list['cell1'].append(d_cell1_kernel)\n",
    "#     print('ELMo cells change per step: cell0: {}, cell1: {}'.format(d_cell0_kernel, d_cell1_kernel))\n",
    "    # Validate\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "    # Get elmo params\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        if TRAIN_ELMO:\n",
    "            layer_coeff, scale = sess.run([elmo_params['layer_coefficients'], elmo_params['scaling']])\n",
    "    losses['valid'].append(loss_valid)\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        print('Train loss = {}'.format(losses['train'][-1]))\n",
    "#         print('Train F1 score = {}'.format(f1_scores['train'][-1]))\n",
    "        if TRAIN_ELMO:\n",
    "            with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "                print('ELMo weights:')\n",
    "                print('Coefficients = {}, scale = {}'.format(layer_coeff, scale))\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            print('ELMo cells change per step: cell0: {:.2f}%, cell1: {:.2f}%'.format(d_cell0_kernel*100, d_cell1_kernel*100))\n",
    "        \n",
    "    if step % valid_step == 0 or step == 1:\n",
    "        print('Valid loss = {}'.format(losses['valid'][-1]))\n",
    "#         print('Valid F1 score = {}'.format(f1_scores['valid'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure()\n",
    "steps = np.arange(1, num_steps+1, 1)\n",
    "plt.plot(steps, losses['train'], c='b', label='train')\n",
    "plt.plot(steps, losses['valid'], c='r', label='valid')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
