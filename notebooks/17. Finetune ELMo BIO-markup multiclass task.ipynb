{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5484,
     "status": "ok",
     "timestamp": 1535754876249,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "8FklJFwcPj4U",
    "outputId": "5b85dbb6-4bf1-456e-bc79-b2757286463c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deeppavlov in /usr/local/lib/python3.6/dist-packages (0.0.6.6)\n",
      "Requirement already satisfied: tqdm==4.23.4 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (4.23.4)\n",
      "Requirement already satisfied: rusenttokenize==0.0.4 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.0.4)\n",
      "Requirement already satisfied: Cython==0.27.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.27.1)\n",
      "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.2.5)\n",
      "Requirement already satisfied: overrides==1.9 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.9)\n",
      "Requirement already satisfied: pandas==0.23.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.23.1)\n",
      "Requirement already satisfied: fuzzywuzzy==0.16.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.16.0)\n",
      "Requirement already satisfied: keras==2.2.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.2.0)\n",
      "Requirement already satisfied: pytelegrambotapi==3.5.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.5.2)\n",
      "Requirement already satisfied: flask-cors==3.0.3 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (3.0.3)\n",
      "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.8.0)\n",
      "Requirement already satisfied: flasgger==0.6.6 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.6.6)\n",
      "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn==0.19.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.19.1)\n",
      "Requirement already satisfied: numpy==1.14.5 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.14.5)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Requirement already satisfied: requests==2.19.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.19.1)\n",
      "Requirement already satisfied: flask==0.12.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.12.2)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.2.5->deeppavlov) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2.5.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->deeppavlov) (2018.5)\n",
      "Requirement already satisfied: keras-applications==1.0.2 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0->deeppavlov) (1.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0->deeppavlov) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0->deeppavlov) (1.0.1)\n",
      "Requirement already satisfied: mistune in /usr/local/lib/python3.6/dist-packages (from flasgger==0.6.6->deeppavlov) (0.8.3)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from flasgger==0.6.6->deeppavlov) (2.6.0)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2.6)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.19.1->deeppavlov) (2018.8.24)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2->deeppavlov) (0.14.1)\n",
      "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2->deeppavlov) (2.10)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2->deeppavlov) (0.24)\n",
      "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2->deeppavlov) (6.7)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.4->flask==0.12.2->deeppavlov) (1.0)\n",
      "Requirement already satisfied: pybind11 in /usr/local/lib/python3.6/dist-packages (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "# For google colab\n",
    "# ! pip install deeppavlov\n",
    "# ! pip install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWrowqeyPaAj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import *\n",
    "import copy\n",
    "import sys\n",
    "from deeppavlov.dataset_readers.ontonotes_reader import OntonotesReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz9piiTQa_Nr"
   },
   "outputs": [],
   "source": [
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGoiwLqLPaAo"
   },
   "outputs": [],
   "source": [
    "TRAIN_ELMO = True\n",
    "TRAIN_ALL_ELMO_PARAMS = True\n",
    "MULTICLASS = True\n",
    "USE_BIO_MARKUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QD4qUfNcPaAr"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    reader = OntonotesReader()\n",
    "    dataset = reader.read(data_path='data/')\n",
    "    # print(dataset.keys())\n",
    "    print('Num of train sentences: {}'.format(len(dataset['train'])))\n",
    "    print('Num of valid sentences: {}'.format(len(dataset['valid'])))\n",
    "    print('Num of test sentences: {}'.format(len(dataset['test'])))\n",
    "    print(dataset['train'][50:60])\n",
    "    return dataset\n",
    "\n",
    "def filter_data_by_ne_type(data:list, ne_types:list, tags2binary=False, preserveBIO=False, keepIfAny=True):\n",
    "    if ne_types == None or len(ne_types) == 0:\n",
    "        return data\n",
    "    data_filtered = []\n",
    "    for tokens,tags in data:\n",
    "        contains_all = True\n",
    "        contains_any = False\n",
    "        tags_norm = [getNeTagMainPart(t) for t in tags]\n",
    "        for ne_type in ne_types:\n",
    "            if not ne_type in tags_norm:\n",
    "                contains_all = False\n",
    "            if ne_type in tags_norm:\n",
    "                contains_any = True\n",
    "        if contains_all or (keepIfAny and contains_any):\n",
    "            if tags2binary:\n",
    "                if preserveBIO:\n",
    "                    tags = [tags[i][:2]+'T' if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "                else:\n",
    "                    tags = ['T' if t in ne_types else 'O' for t in tags_norm]\n",
    "            else:\n",
    "                if preserveBIO:\n",
    "                    tags = [tags[i][:2]+t if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "                else:\n",
    "                    tags = [t if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "            data_filtered.append((tokens,tags))\n",
    "    return data_filtered\n",
    "\n",
    "def filter_dataset_by_ne_types(dataset: list, ne_types, tags2binary=True, preserveBIO=False, keepIfAny=True):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    if not isinstance(ne_types, list):\n",
    "        ne_types = [ne_types]\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        dataset[dataset_type] = filter_data_by_ne_type(dataset[dataset_type], ne_types, tags2binary=tags2binary, preserveBIO=preserveBIO)\n",
    "        print('Num of {} sentences: {}'.format(dataset_type, len(dataset[dataset_type])))\n",
    "    return dataset\n",
    "\n",
    "def get_data_sample(data, n_samples: int):\n",
    "    indices = np.random.choice(len(data), size=n_samples, replace=False)\n",
    "    return split_tokens_tags([data[i] for i in indices])\n",
    "\n",
    "def get_tokens_len(tokens):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    return [len(seq) for seq in tokens]\n",
    "\n",
    "def to_lower_case(tokens:list):\n",
    "    tokens_lower = []\n",
    "    for seq in tokens:\n",
    "        tokens_lower.append([])\n",
    "        for token in seq:\n",
    "            tokens_lower[-1].append(token.lower())\n",
    "    return tokens_lower\n",
    "\n",
    "def add_padding(tokens:list):\n",
    "    if isinstance(tokens[0], str):\n",
    "        return tokens, len(tokens)\n",
    "    elif isinstance(tokens[0], list):\n",
    "        tokens = copy.deepcopy(tokens)\n",
    "        max_len = 0\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_len:\n",
    "                max_len = len(seq)\n",
    "        for seq in tokens:\n",
    "            i = len(seq)\n",
    "            while i < max_len:\n",
    "                seq.append('')\n",
    "                i += 1\n",
    "        return tokens\n",
    "    else:\n",
    "        raise Exception('tokens should be either list of strings or list of lists of strings')\n",
    "  \n",
    "def getNeTagMainPart(tag:str):\n",
    "    return tag[2:] if len(tag) > 2 else tag\n",
    "\n",
    "def tags2binaryFlat(tags):\n",
    "    return np.array([1 if t == 'T' or (len(t) > 2 and t[2:] == 'T') else 0 for seq in tags for t in seq])\n",
    "\n",
    "def tagsEncodePadded(tags:list, binary=True, tag2idx=None):\n",
    "    if tag2idx:\n",
    "        binary = False\n",
    "    if isinstance(tags[0], str):\n",
    "        tags = [tags]\n",
    "    n_sentences = len(tags)\n",
    "    tokens_length = get_tokens_len(tags)\n",
    "    max_len = np.max(tokens_length)\n",
    "    y = np.zeros((n_sentences, max_len))\n",
    "    for i, sen in enumerate(tags):\n",
    "        for j, tag in enumerate(sen):\n",
    "            if binary: \n",
    "                y[i][j] = 1 if tags[i][j] != 'O' else 0\n",
    "            else:\n",
    "                if tag2idx:\n",
    "#                     tag_name = tag if USE_BIO_MARKUP else getNeTagMainPart(tag)\n",
    "                    tag_name = tag   # in case when BIO markup was deleted from data\n",
    "                    y[i][j] = tag2idx[tag_name]\n",
    "                else:\n",
    "                    raise Exception('tag2idx dictionary should be provided')\n",
    "    return y\n",
    "\n",
    "def get_matrices(tokens, tags, embedder):\n",
    "    return (embeddings2feat_mat(embedder.embed(tokens), get_tokens_len(tokens)),\n",
    "           tags2binaryFlat(tags))\n",
    "  \n",
    "def split_tokens_tags(dataset: list):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for sample in dataset:\n",
    "        tokens.append(sample[0])\n",
    "        tags.append(sample[1])\n",
    "    return tokens, tags\n",
    "\n",
    "def count_tags(dataset: list):\n",
    "    tag_counts = {}\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        tag_counts[data_type] = {}\n",
    "        for sen in dataset[data_type]:\n",
    "            tags = sen[1]\n",
    "            for tag in tags:\n",
    "                if tag_counts[data_type].get(tag):\n",
    "                    tag_counts[data_type][tag] += 1\n",
    "                else:\n",
    "                    tag_counts[data_type][tag] = 1\n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11467,
     "status": "ok",
     "timestamp": 1535754891200,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "gKtBm_jBPaAt",
    "outputId": "b9bf0ed3-d79a-45e2-be3c-3de8fec4c838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train sentences: 75187\n",
      "Num of valid sentences: 9603\n",
      "Num of test sentences: 9479\n",
      "[(['Actions', 'had', 'to', 'be', 'taken', 'to', 'break', 'through', 'the', 'blockade', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'a', 'night', 'in', 'late', 'July', '1940', ',', 'the', 'atmosphere', 'in', 'Zhuanbi', 'Village', 'in', 'Shaanxi', 'was', 'unusual', '.'], ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'B-GPE', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'that', 'dark', 'night', ',', 'everyone', 'was', 'sleeping', 'when', 'human', 'voices', 'and', 'neighing', 'horses', 'were', 'heard', 'within', 'the', 'village', '.'], ['O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['People', 'all', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Did', 'something', 'happen', '?'], ['O', 'O', 'O', 'O']), (['Some', 'folks', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Opening', 'the', 'street', 'gate', ',', 'they', 'saw', 'a', 'soldier', 'standing', 'by', 'the', 'gate', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Folks', ',', 'go', 'back', ',', 'go', 'back', ',', 'nothing', 'is', 'wrong', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Our', 'troops', 'are', 'just', 'going', 'to', 'stay', 'here', 'for', 'the', 'night', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n",
      "NE types used for training:\n",
      "['GPE', 'DATE', 'ORG', 'EVENT', 'LOC', 'FAC', 'CARDINAL', 'QUANTITY', 'NORP', 'ORDINAL', 'WORK_OF_ART', 'LANGUAGE', 'TIME', 'PRODUCT', 'MONEY', 'LAW', 'PERCENT']\n",
      "17 in total\n",
      "Num of train sentences: 28872\n",
      "Num of valid sentences: 3975\n",
      "Num of test sentences: 4026\n",
      "Number of sentences in filtered dataset: train: 28872, valid: 3975, test: 4026\n",
      "Tag counts:\n",
      "{'train': {'O': 597244, 'B-ORG': 12820, 'I-ORG': 18246, 'B-WORK_OF_ART': 974, 'I-WORK_OF_ART': 2400, 'B-LOC': 1514, 'I-LOC': 1395, 'B-CARDINAL': 7367, 'B-EVENT': 748, 'I-EVENT': 1605, 'B-NORP': 6870, 'B-GPE': 15405, 'I-GPE': 3679, 'B-DATE': 10922, 'I-DATE': 13333, 'B-FAC': 860, 'I-FAC': 1467, 'B-QUANTITY': 657, 'I-QUANTITY': 1235, 'B-ORDINAL': 1640, 'B-TIME': 1233, 'I-TIME': 1507, 'I-CARDINAL': 2289, 'B-PRODUCT': 606, 'I-PRODUCT': 576, 'I-NORP': 446, 'B-PERCENT': 1763, 'I-PERCENT': 2498, 'B-MONEY': 2434, 'I-MONEY': 4920, 'B-LAW': 282, 'I-LAW': 785, 'B-LANGUAGE': 304, 'I-LANGUAGE': 13, 'I-ORDINAL': 5}, 'valid': {'O': 80615, 'B-DATE': 1507, 'I-DATE': 1809, 'B-GPE': 2268, 'I-GPE': 555, 'B-ORDINAL': 232, 'B-ORG': 1740, 'B-FAC': 115, 'I-FAC': 203, 'I-ORG': 2336, 'B-QUANTITY': 100, 'I-QUANTITY': 209, 'B-LOC': 204, 'I-LOC': 188, 'B-CARDINAL': 938, 'B-NORP': 847, 'B-LAW': 40, 'I-LAW': 84, 'I-CARDINAL': 290, 'B-MONEY': 274, 'I-MONEY': 587, 'B-TIME': 214, 'B-EVENT': 143, 'I-EVENT': 272, 'B-WORK_OF_ART': 142, 'I-WORK_OF_ART': 334, 'I-TIME': 260, 'B-PERCENT': 177, 'I-PERCENT': 258, 'I-NORP': 44, 'B-PRODUCT': 72, 'I-PRODUCT': 129, 'B-LANGUAGE': 33, 'I-ORDINAL': 4}, 'test': {'O': 82515, 'B-NORP': 841, 'B-ORG': 1795, 'I-ORG': 2406, 'B-DATE': 1602, 'B-GPE': 2240, 'I-DATE': 2011, 'B-FAC': 135, 'I-FAC': 213, 'I-GPE': 628, 'B-CARDINAL': 935, 'B-TIME': 212, 'I-TIME': 255, 'B-ORDINAL': 195, 'B-EVENT': 63, 'I-EVENT': 130, 'I-CARDINAL': 331, 'B-QUANTITY': 105, 'I-QUANTITY': 206, 'B-PERCENT': 349, 'I-PERCENT': 523, 'B-LOC': 179, 'I-LOC': 180, 'B-WORK_OF_ART': 166, 'I-WORK_OF_ART': 337, 'B-MONEY': 314, 'I-MONEY': 685, 'B-LAW': 40, 'I-LAW': 106, 'I-NORP': 160, 'I-ORDINAL': 4, 'B-PRODUCT': 76, 'I-PRODUCT': 69, 'B-LANGUAGE': 22}}\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = read_data()\n",
    "ne_types_holdout = ['PERSON']\n",
    "ne_types_all = ['GPE','DATE','ORG','EVENT','LOC','FAC','CARDINAL','QUANTITY','NORP','ORDINAL','WORK_OF_ART', 'LANGUAGE', 'TIME', 'PRODUCT', 'MONEY', 'LAW', 'PERCENT', 'PERSON']\n",
    "ne_types = [t for t in ne_types_all if t not in ne_types_holdout]\n",
    "# ne_types = ['PERSON', 'ORG']\n",
    "# ne_types = ['LOC', 'EVENT']\n",
    "print('NE types used for training:')\n",
    "print(ne_types)\n",
    "print('{} in total'.format(len(ne_types)))\n",
    "dataset = filter_dataset_by_ne_types(dataset_orig, ne_types, tags2binary=False, preserveBIO=USE_BIO_MARKUP, keepIfAny=True)\n",
    "print('Number of sentences in filtered dataset: train: {}, valid: {}, test: {}'.format(len(dataset['train']), len(dataset['valid']), len(dataset['test'])))\n",
    "tag_counts = count_tags(dataset)\n",
    "print('Tag counts:')\n",
    "print(tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1535754891874,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "T74bOXjxPaAw",
    "outputId": "8b445d09-09ec-40db-aa2d-193270babf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'respectfully', 'invite', 'you', 'to', 'watch', 'a', 'special', 'edition', 'of', 'Across', 'China', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O']), (['WW', 'II', 'Landmarks', 'on', 'the', 'Great', 'Earth', 'of', 'China', ':', 'Eternal', 'Memories', 'of', 'Taihang', 'Mountain'], ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']), (['Standing', 'tall', 'on', 'Taihang', 'Mountain', 'is', 'the', 'Monument', 'to', 'the', 'Hundred', 'Regiments', 'Offensive', '.'], ['O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O']), (['It', 'is', 'composed', 'of', 'a', 'primary', 'stele', ',', 'secondary', 'steles', ',', 'a', 'huge', 'round', 'sculpture', 'and', 'beacon', 'tower', ',', 'and', 'the', 'Great', 'Wall', ',', 'among', 'other', 'things', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O']), (['A', 'primary', 'stele', ',', 'three', 'secondary', 'steles', ',', 'and', 'two', 'inscribed', 'steles', '.'], ['O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O']), (['The', 'Hundred', 'Regiments', 'Offensive', 'was', 'the', 'campaign', 'of', 'the', 'largest', 'scale', 'launched', 'by', 'the', 'Eighth', 'Route', 'Army', 'during', 'the', 'War', 'of', 'Resistance', 'against', 'Japan', '.'], ['B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O']), (['This', 'campaign', 'broke', 'through', 'the', 'Japanese', 'army', \"'s\", 'blockade', 'to', 'reach', 'base', 'areas', 'behind', 'enemy', 'lines', ',', 'stirring', 'up', 'anti-Japanese', 'spirit', 'throughout', 'the', 'nation', 'and', 'influencing', 'the', 'situation', 'of', 'the', 'anti-fascist', 'war', 'of', 'the', 'people', 'worldwide', '.'], ['O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['This', 'is', 'Zhuanbi', 'Village', ',', 'Wuxiang', 'County', 'of', 'Shanxi', 'Province', ',', 'where', 'the', 'Eighth', 'Route', 'Army', 'was', 'headquartered', 'back', 'then', '.'], ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'I-GPE', 'I-GPE', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O']), (['This', 'map', 'was', 'the', 'Eighth', 'Route', 'Army', \"'s\", 'depiction', 'of', 'the', 'Mediterranean', 'Sea', 'situation', 'at', 'that', 'time', '.'], ['O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O']), (['This', 'map', 'reflected', 'the', 'European', 'battlefield', 'situation', '.'], ['O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1535754892805,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "KkFrEV5kPaAz",
    "outputId": "103de6d6-d2ac-4f52-c3f9-fc5d76d8c336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-GPE': 1, 'I-GPE': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-EVENT': 7, 'I-EVENT': 8, 'B-LOC': 9, 'I-LOC': 10, 'B-FAC': 11, 'I-FAC': 12, 'B-CARDINAL': 13, 'I-CARDINAL': 14, 'B-QUANTITY': 15, 'I-QUANTITY': 16, 'B-NORP': 17, 'I-NORP': 18, 'B-ORDINAL': 19, 'I-ORDINAL': 20, 'B-WORK_OF_ART': 21, 'I-WORK_OF_ART': 22, 'B-LANGUAGE': 23, 'I-LANGUAGE': 24, 'B-TIME': 25, 'I-TIME': 26, 'B-PRODUCT': 27, 'I-PRODUCT': 28, 'B-MONEY': 29, 'I-MONEY': 30, 'B-LAW': 31, 'I-LAW': 32, 'B-PERCENT': 33, 'I-PERCENT': 34}\n",
      "['O', 'B-GPE', 'I-GPE', 'B-DATE', 'I-DATE', 'B-ORG', 'I-ORG', 'B-EVENT', 'I-EVENT', 'B-LOC', 'I-LOC', 'B-FAC', 'I-FAC', 'B-CARDINAL', 'I-CARDINAL', 'B-QUANTITY', 'I-QUANTITY', 'B-NORP', 'I-NORP', 'B-ORDINAL', 'I-ORDINAL', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LANGUAGE', 'I-LANGUAGE', 'B-TIME', 'I-TIME', 'B-PRODUCT', 'I-PRODUCT', 'B-MONEY', 'I-MONEY', 'B-LAW', 'I-LAW', 'B-PERCENT', 'I-PERCENT']\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {}\n",
    "idx2tag = []\n",
    "tag2idx['O'] = 0\n",
    "idx2tag.append('O')\n",
    "i = 1\n",
    "for tag in ne_types:\n",
    "    if USE_BIO_MARKUP:\n",
    "        tag2idx['B-'+tag] = i\n",
    "        tag2idx['I-'+tag] = i+1\n",
    "        idx2tag.append('B-'+tag)\n",
    "        idx2tag.append('I-'+tag)\n",
    "        i += 2\n",
    "    else:\n",
    "        tag2idx[tag] = i\n",
    "        idx2tag.append(tag)\n",
    "        i += 1\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZK3jHAGzPaA2"
   },
   "outputs": [],
   "source": [
    "from deeppavlov.core.common.check_gpu import GPU_AVAILABLE\n",
    "INITIALIZER = tf.contrib.layers.xavier_initializer\n",
    "\n",
    "def cudnn_lstm_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "                       initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "\n",
    "    if GPU_AVAILABLE:\n",
    "        return cudnn_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                          seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "    log.info('\\nWarning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. '\n",
    "             'It is okay for inference mode, but '\n",
    "             'if you train your model with this cell it could NOT be used with '\n",
    "             'tf.contrib.cudnn_rnn.CudnnLSTMCell later. '\n",
    "             )\n",
    "\n",
    "    return cudnn_compatible_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                                 seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "def cudnn_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "               initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "    \"\"\" Fast CuDNN LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            seq_lengths: tensor of sequence lengths with dimension [B]\n",
    "            initial_h: optional initial hidden state, masks trainable_initial_states\n",
    "                if provided\n",
    "            initial_c: optional initial cell state, masks trainable_initial_states\n",
    "                if provided\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=n_layers,\n",
    "                                              num_units=n_hidden)\n",
    "        if trainable_initial_states:\n",
    "            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n",
    "            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n",
    "            init_c = tf.get_variable('init_c', [n_layers, 1, n_hidden])\n",
    "            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n",
    "        else:\n",
    "            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n",
    "\n",
    "        initial_h = initial_h or init_h\n",
    "        initial_c = initial_c or init_c\n",
    "\n",
    "        h, (h_last, c_last) = lstm(tf.transpose(units, (1, 0, 2)), (initial_h, initial_c))\n",
    "        h = tf.transpose(h, (1, 0, 2))\n",
    "        h_last = tf.reshape(h_last, shape=(-1, n_hidden))\n",
    "        c_last = tf.reshape(c_last, shape=(-1, n_hidden))\n",
    "\n",
    "        # Extract last states if they are provided\n",
    "        if seq_lengths is not None:\n",
    "            indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1)\n",
    "            h_last = tf.gather_nd(h, indices)\n",
    "\n",
    "        return h, (h_last, c_last)\n",
    "\n",
    "def cudnn_bi_lstm(units,\n",
    "                  n_hidden,\n",
    "                  seq_lengths=None,\n",
    "                  n_layers=1,\n",
    "                  trainable_initial_states=False,\n",
    "                  name='cudnn_bi_lstm',\n",
    "                  reuse=False):\n",
    "    \"\"\" Fast CuDNN Bi-LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            seq_lengths: number of tokens in each sample in the batch\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        if seq_lengths is None:\n",
    "            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n",
    "        with tf.variable_scope('Forward'):\n",
    "            h_fw, (h_fw_last, c_fw_last) = cudnn_lstm_wrapper(units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "        with tf.variable_scope('Backward'):\n",
    "            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "            h_bw, (h_bw_last, c_bw_last) = cudnn_lstm_wrapper(reversed_units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "        return (h_fw, h_bw), ((h_fw_last, c_fw_last), (h_bw_last, c_bw_last))\n",
    "\n",
    "def bi_rnn(units: tf.Tensor,\n",
    "           n_hidden: List,\n",
    "           cell_type='gru',\n",
    "           seq_lengths=None,\n",
    "           trainable_initial_states=False,\n",
    "           use_peepholes=False,\n",
    "           name='Bi-'):\n",
    "    \"\"\" Bi directional recurrent neural network. GRU or LSTM\n",
    "\n",
    "        Args:\n",
    "            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n",
    "            n_hidden_list: list with number of hidden units at the ouput of each layer\n",
    "            seq_lengths: length of sequences for different length sequences in batch\n",
    "                can be None for maximum length as a length for every sample in the batch\n",
    "            cell_type: 'lstm' or 'gru'\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            use_peepholes: whether to use peephole connections (only 'lstm' case affected)\n",
    "            name: what variable_scope to use for the network parameters\n",
    "            add_l2_losses: whether to add l2 losses on network kernels to\n",
    "                tf.GraphKeys.REGULARIZATION_LOSSES or not\n",
    "        Returns:\n",
    "            units: tensor at the output of the last recurrent layer\n",
    "                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n",
    "            last_units: tensor of last hidden states for GRU and tuple\n",
    "                of last hidden stated and last cell states for LSTM\n",
    "                dimensionality of cell states and hidden states are\n",
    "                similar and equal to [B x 2 * H], where B - batch\n",
    "                size and H is number of hidden units\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(name + '_' + cell_type.upper()):\n",
    "        if cell_type == 'gru':\n",
    "            forward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "                initial_state_bw = tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        elif cell_type == 'lstm':\n",
    "            forward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_fw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "                initial_state_bw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_bw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        else:\n",
    "            raise RuntimeError('cell_type must be either \"gru\" or \"lstm\"s')\n",
    "        (rnn_output_fw, rnn_output_bw), (fw, bw) = \\\n",
    "            tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                            backward_cell,\n",
    "                                            units,\n",
    "                                            dtype=tf.float32,\n",
    "                                            sequence_length=seq_lengths,\n",
    "                                            initial_state_fw=initial_state_fw,\n",
    "                                            initial_state_bw=initial_state_bw)\n",
    "    kernels = [var for var in forward_cell.trainable_variables +\n",
    "               backward_cell.trainable_variables if 'kernel' in var.name]\n",
    "    for kernel in kernels:\n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(kernel))\n",
    "    return (rnn_output_fw, rnn_output_bw), (fw, bw)\n",
    "\n",
    "def build_cudnn_rnn(units, mask, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        with tf.variable_scope(cell_type.upper() + '_' + str(n)):\n",
    "            if cell_type.lower() == 'lstm':\n",
    "                units, _ = cudnn_bi_lstm(units, n_hidden, sequence_lengths)\n",
    "            elif cell_type.lower() == 'gru':\n",
    "                units, _ = cudnn_bi_gru(units, n_hidden, sequence_lengths)\n",
    "            else:\n",
    "                raise RuntimeError('Wrong cell type \"{}\"! Only \"gru\" and \"lstm\"!'.format(cell_type))\n",
    "            units = tf.concat(units, -1)\n",
    "            if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "                units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_rnn(units, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        units, _ = bi_rnn(units, n_hidden, cell_type=cell_type, name='Layer_' + str(n))\n",
    "        units = tf.concat(units, -1)\n",
    "        if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "            units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_top(units, n_tags=18*2+1, top_dropout:bool=False, two_dense_on_top:bool=False, n_hidden=128):\n",
    "    if top_dropout:\n",
    "        units = variational_dropout(units, dropout_ph)\n",
    "    if two_dense_on_top:\n",
    "        units = tf.layers.dense(units, n_hidden, activation=tf.nn.relu,\n",
    "                                kernel_initializer=INITIALIZER(),\n",
    "                                kernel_regularizer=tf.nn.l2_loss)\n",
    "    logits = tf.layers.dense(units, n_tags, activation=None,\n",
    "                             kernel_initializer=INITIALIZER(),\n",
    "                             kernel_regularizer=tf.nn.l2_loss)\n",
    "    return logits\n",
    "\n",
    "def build_train_predict(logits, n_tags, mask, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg):\n",
    "    res = {}\n",
    "    if use_crf:\n",
    "        sequence_lengths = tf.reduce_sum(mask, axis=1)\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(logits, y_ph, sequence_lengths)\n",
    "        loss_tensor = -log_likelihood\n",
    "        res['transition_params'] = transition_params\n",
    "    else:\n",
    "        ground_truth_labels = tf.one_hot(y_ph, n_tags)\n",
    "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_labels, logits=logits)\n",
    "        loss_tensor = loss_tensor * mask\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        res['y_pred'] = y_pred\n",
    "\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "\n",
    "    # L2 regularization\n",
    "    if l2_reg > 0:\n",
    "        loss += l2_reg * tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    res['loss'] = loss\n",
    "        \n",
    "    # optimizer = partial(tf.train.MomentumOptimizer, momentum=0.9, use_nesterov=True)\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    train_op = get_train_op(loss, learning_rate_ph, optimizer, clip_norm=clip_grad_norm)\n",
    "    res['train_op'] = train_op\n",
    "    return res\n",
    "\n",
    "def predict_no_crf(y_pred, mask_ph, feed_dict):\n",
    "    pred_idxs, mask = sess.run([y_pred, mask_ph], feed_dict)\n",
    "\n",
    "    # Filter by sequece length\n",
    "    sequence_lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "    pred = []\n",
    "    for utt, l in zip(pred_idxs, sequence_lengths):\n",
    "        pred.append(utt[:l])\n",
    "    return pred\n",
    "\n",
    "def predict_crf(logits, transition_params, mask_ph, feed_dict):\n",
    "    logits, trans_params, mask = sess.run([logits,\n",
    "                                           transition_params,\n",
    "                                           mask_ph],\n",
    "                                           feed_dict=feed_dict)\n",
    "    sequence_lengths = np.maximum(np.sum(mask, axis=1).astype(np.int32), 1)\n",
    "    # iterate over the sentences because no batching in viterbi_decode\n",
    "    pred = []\n",
    "    for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "        logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
    "        viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "        pred += [viterbi_seq]\n",
    "    return pred\n",
    "\n",
    "def get_train_op(loss,\n",
    "                 learning_rate,\n",
    "                 optimizer=None,\n",
    "                 clip_norm=None,\n",
    "                 learnable_scopes=None,\n",
    "                 optimizer_scope_name=None,\n",
    "                 trainable_vars:list=None):\n",
    "    \"\"\" Get train operation for given loss\n",
    "\n",
    "    Args:\n",
    "        loss: loss, tf tensor or scalar\n",
    "        learning_rate: scalar or placeholder\n",
    "        clip_norm: clip gradients norm by clip_norm\n",
    "        learnable_scopes: which scopes are trainable (None for all)\n",
    "        optimizer: instance of tf.train.Optimizer, default Adam\n",
    "\n",
    "    Returns:\n",
    "        train_op\n",
    "    \"\"\"\n",
    "    if optimizer_scope_name is None:\n",
    "        opt_scope = tf.variable_scope('Optimizer')\n",
    "    else:\n",
    "        opt_scope = tf.variable_scope(optimizer_scope_name)\n",
    "    with opt_scope:\n",
    "        if learnable_scopes is None:\n",
    "            variables_to_train = tf.global_variables()\n",
    "        else:\n",
    "            variables_to_train = []\n",
    "            for scope_name in learnable_scopes:\n",
    "                for var in tf.global_variables():\n",
    "                    if scope_name in var.name:\n",
    "                        variables_to_train.append(var)\n",
    "        if trainable_vars:\n",
    "            variables_to_train = trainable_vars\n",
    "            \n",
    "        if optimizer is None:\n",
    "            optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "        # For batch norm it is necessary to update running averages\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            opt = optimizer(learning_rate)\n",
    "            grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n",
    "            if clip_norm is not None:\n",
    "                grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                  for grad, var in grads_and_vars] #  if grad is not None\n",
    "            train_op = opt.apply_gradients(grads_and_vars)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cfEVVG1PaA4"
   },
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size=None, binaryTags=True, tag2idx=None):\n",
    "    if not batch_size:\n",
    "        batch_size = len(dataset)\n",
    "    tokens, tags = get_data_sample(dataset, batch_size)\n",
    "    mask = make_mask(tokens)\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    tokens = add_padding(tokens)\n",
    "    y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "    return tokens, tags, mask, y\n",
    "\n",
    "def make_mask(seq_list):\n",
    "  seq_count = len(seq_list)\n",
    "  seq_length = [len(s) for s in seq_list]\n",
    "  max_len = np.max(seq_length)\n",
    "  mask = np.zeros((seq_count, max_len), dtype=int)\n",
    "  seq_length = np.tile(np.expand_dims(seq_length, axis=-1), (1, max_len))\n",
    "  range_ar = np.tile(np.arange(1, max_len+1, 1), (seq_count, 1))\n",
    "  mask[range_ar <= seq_length] = 1\n",
    "  return mask\n",
    "\n",
    "def flatten_with_mask(seq_mat, mask):\n",
    "  return seq_mat[mask == 1]\n",
    "\n",
    "def concatenate_arrays(ar_list):\n",
    "  return np.concatenate(ar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHZPI7b3PaA7"
   },
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, data):\n",
    "        self.data = {\n",
    "            'train': data['train'],\n",
    "            'valid': data['valid'],\n",
    "            'test': data['test']\n",
    "        }\n",
    "        \n",
    "    def get_samples_count(self, data_type='train'):\n",
    "        return len(self.data[data_type])\n",
    "\n",
    "    def gen_batches(self, batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=None):\n",
    "        indices = np.arange(len(self.data[data_type]))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        n = indices.size\n",
    "        k = 0\n",
    "        while k < n:\n",
    "            top = k + batch_size\n",
    "            if top > n: \n",
    "                top = n\n",
    "            indices_batch = indices[k:top]\n",
    "            tokens,tags = split_tokens_tags([self.data[data_type][i] for i in indices_batch])\n",
    "            mask = make_mask(tokens)\n",
    "            tokens_length = get_tokens_len(tokens)\n",
    "            tokens = add_padding(tokens)\n",
    "            y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "            yield tokens, tags, mask, y\n",
    "            k += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFG9BPwlPaA9"
   },
   "outputs": [],
   "source": [
    "def eval_valid(batch_generator, tag2idx):\n",
    "    pred_total = []\n",
    "    true_total = []\n",
    "    loss_valid_total = 0\n",
    "    for i, (tokens_valid, tags_valid, mask_valid, y_valid) in enumerate(batch_generator.gen_batches(32, data_type='valid', shuffle=False, binaryTags=False, tag2idx=tag2idx)):\n",
    "        feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "        y_valid_flat = flatten_with_mask(y_valid, mask_valid)\n",
    "        loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "        if use_crf:\n",
    "            pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "        else:\n",
    "            pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "#         print('y_true: {}'.format(y_valid))\n",
    "#         print('y_pred: {}'.format(pred))\n",
    "        pred = concatenate_arrays(pred)\n",
    "        loss_valid_total += loss_valid\n",
    "        pred_total = np.concatenate([pred_total, pred])\n",
    "        true_total = np.concatenate([true_total, y_valid_flat])\n",
    "#     print(np.unique(true_total))\n",
    "#     print(np.unique(pred_total))\n",
    "#     print(true_total.size == pred_total.size)\n",
    "#     a = np.arange(true_total.size)\n",
    "#     print(a[true_total == 2])\n",
    "#     print(a[pred_total == 2])\n",
    "    n_tags = len(tag2idx.keys())\n",
    "    f1_valid = f1_score(true_total, pred_total, average=None, labels=list(range(1,n_tags)))\n",
    "#     print(f1_valid)\n",
    "    loss_valid_total = loss_valid_total/(i+1)\n",
    "    \n",
    "    return {'loss': loss_valid_total, 'f1': f1_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2WmzRJuPaBB"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiXGWeaSPaBD"
   },
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=TRAIN_ELMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1535754900840,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "fIemR3FYPaBI",
    "outputId": "d1e86786-96ed-42c0-b74d-9d5c434f53be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/aggregation/weights:0' shape=(3,) dtype=float32>, <tf.Variable 'module/aggregation/scaling:0' shape=() dtype=float32>]\n",
      "{'layer_coefficients': <tf.Variable 'module/aggregation/weights:0' shape=(3,) dtype=float32>, 'scaling': <tf.Variable 'module/aggregation/scaling:0' shape=() dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "print(tf.trainable_variables())\n",
    "if(TRAIN_ELMO):\n",
    "    elmo_coef = {'layer_coefficients': tf.trainable_variables()[-2], 'scaling': tf.trainable_variables()[-1]}\n",
    "    print(elmo_coef)\n",
    "elmo_vars = tf.trainable_variables()\n",
    "elmo_vars_coef = list(elmo_coef.values())\n",
    "elmo_vars_cell_weights = [v for v in elmo_vars if v not in elmo_vars_coef]\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}\n",
    "if TRAIN_ALL_ELMO_PARAMS:\n",
    "    cell0_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0']\n",
    "    cell1_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_1Gj9SCPaBK"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "use_cudnn_rnn = True\n",
    "l2_reg = 0\n",
    "n_hidden_list = (128,)\n",
    "cell_type = 'lstm'\n",
    "n_tags = len(idx2tag)\n",
    "use_crf = True\n",
    "clip_grad_norm = 5.0\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiD7H4jnPaBN"
   },
   "source": [
    "### Build computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9Ja_jdYPaBN"
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "tokens_input_ph = tf.placeholder(shape=[None, None], dtype=tf.string)\n",
    "# tokens_length_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "y_ph = tf.placeholder(shape=[None, None], dtype=tf.int32, name='y_ph')\n",
    "learning_rate_ph = tf.placeholder_with_default(learning_rate, shape=[], name='learning_rate')\n",
    "dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name='dropout')\n",
    "training_ph = tf.placeholder_with_default(False, shape=[], name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PElhM3v9PaBQ"
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(inp: dict, train=True):\n",
    "    feed_dict = {learning_rate_ph: learning_rate, dropout_ph: dropout_keep_prob if train else 1.0, training_ph: train}\n",
    "    feed_dict.update(inp)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7763,
     "status": "ok",
     "timestamp": 1535754911225,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "2k__QsnjPaBU",
    "outputId": "c5837ecf-def7-4a1e-fb33-eae4a87e691d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-31 22:35:04.141 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "y_pred = None\n",
    "transition_params = None\n",
    "tokens_length = tf.to_int32(tf.reduce_sum(mask_ph, axis=1))\n",
    "emb = elmo(inputs={\"tokens\": tokens_input_ph,\n",
    "                    \"sequence_len\": tokens_length},\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)['elmo']\n",
    "# mask = tf.sequence_mask(lengths=tokens_length_ph, dtype=tf.float32)\n",
    "features = emb\n",
    "if use_cudnn_rnn:\n",
    "    units = build_cudnn_rnn(features, mask_ph, n_hidden_list, cell_type)\n",
    "else:\n",
    "    units = build_rnn(features, n_hidden_list, cell_type)\n",
    "\n",
    "logits = build_top(units, n_tags=n_tags)\n",
    "\n",
    "out_dict = build_train_predict(logits, n_tags, mask_ph, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg)\n",
    "train_op_all = out_dict['train_op']\n",
    "loss = out_dict['loss']\n",
    "if use_crf:\n",
    "    transition_params = out_dict['transition_params']\n",
    "else:\n",
    "    y_pred = out_dict['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1535754911923,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "Girz7LPfPaBZ",
    "outputId": "108a2794-fd9a-43e6-e6e9-9d75d3b5eb46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'LSTM_0/cudnn_bi_lstm/Forward/cudnn_lstm/cudnn_lstm/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'LSTM_0/cudnn_bi_lstm/Backward/cudnn_lstm/cudnn_lstm/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(256, 35) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(35,) dtype=float32_ref>, <tf.Variable 'transitions:0' shape=(35, 35) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "model_vars = [v for v in all_vars if v not in elmo_vars]\n",
    "print(model_vars)\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8061,
     "status": "ok",
     "timestamp": 1535754920250,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "nYhuNEucPaBe",
    "outputId": "81407e25-7cb5-4e47-bc96-8174ada4ee7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Optimizers for different parameters\n",
    "with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE):\n",
    "    train_op_model = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=model_vars)\n",
    "    train_op_elmo = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars)\n",
    "    train_op_elmo_coef = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_coef)\n",
    "    train_op_elmo_cell_weights = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_cell_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zC9DxF3EPaBk"
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('./graph/bilstm_crf_elmo_bio_multi', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlqOMRZ_kVuf"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6190,
     "status": "ok",
     "timestamp": 1535754931375,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "7m5dO-N2PaBp",
    "outputId": "6ed6a02d-2b7b-4fd8-9d8e-8a1ce5f21aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 236,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run([initialize_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqnoc3KbPaBt"
   },
   "outputs": [],
   "source": [
    "dataset_iterator = DatasetIterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNPK6IDSPaBv"
   },
   "outputs": [],
   "source": [
    "valid_sen_size = 100\n",
    "tokens_valid, tags_valid, mask_valid, y_valid = get_batch(dataset['valid'], valid_sen_size, binaryTags=False, tag2idx=tag2idx)\n",
    "feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "y_valid_flat = flatten_with_mask(y_valid, mask_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs3vYbzrPaBy"
   },
   "outputs": [],
   "source": [
    "# print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3KZRiO9PaB3"
   },
   "outputs": [],
   "source": [
    "# print(tags_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1535754935298,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "PyIP0uJyPaB5",
    "outputId": "ac8f38b8-f010-42f3-86b5-d62ec1083968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 21. 22. 25. 26. 27. 28. 29. 30. 33. 34.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dOdH8vdPaB8"
   },
   "outputs": [],
   "source": [
    "# training_schedule = [{'train_op': train_op_model, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_cell_weights, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_coef, 'n_epochs': 200, 'lr': 1e-2}]\n",
    "training_schedule = [{'train_op': train_op_model, 'n_epochs': 10, 'lr': 3e-3}, {'train_op': train_op_elmo_cell_weights, 'n_epochs': 10, 'lr': 1e-3}, {'train_op': train_op_elmo_coef, 'n_epochs': 10, 'lr': 1e-2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "APd8Zl-9PaB-",
    "outputId": "5141d44f-1b23-4529-f6b5-8edc727f2c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "[====================] 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.48364925384521484\n",
      "ELMo weights:\n",
      "Coefficients = [0. 0. 0.], scale = 1.0\n",
      "ELMo cells change per epoch: cell0: 0.00%, cell1: 0.00%\n",
      "Valid loss = 2.074375849723816\n",
      "Valid F1 score = 0.709625909908958\n",
      "Epoch 2/12\n",
      "[====================] 100%\n",
      "Train loss = 2.649805784225464\n",
      "ELMo weights:\n",
      "Coefficients = [0. 0. 0.], scale = 1.0\n",
      "ELMo cells change per epoch: cell0: 0.00%, cell1: 0.00%\n",
      "Valid loss = 1.828318841457367\n",
      "Valid F1 score = 0.7149316358700278\n",
      "Epoch 3/12\n",
      "[====================] 100%\n",
      "Train loss = 0.746431827545166\n",
      "ELMo weights:\n",
      "Coefficients = [0. 0. 0.], scale = 1.0\n",
      "ELMo cells change per epoch: cell0: 0.00%, cell1: 0.00%\n",
      "Valid loss = 1.8573682222366332\n",
      "Valid F1 score = 0.7260644754533524\n",
      "Epoch 4/12\n",
      "[=                   ] 5%"
     ]
    }
   ],
   "source": [
    "num_epochs = np.sum([s['n_epochs'] for s in training_schedule])\n",
    "stage = 0\n",
    "n_epochs_prev_stages = 0\n",
    "display_epoch = 1\n",
    "valid_epoch = 1\n",
    "losses_train = []\n",
    "losses_epoch = {'train': [], 'valid': []}\n",
    "f1_scores = {'train': [], 'valid': []}\n",
    "best_valid_f1 = 0\n",
    "d_elmo_cells_list = {'cell0':[], 'cell1':[]}\n",
    "n_batches_train = ceil(len(dataset['train'])/batch_size)\n",
    "step = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "    if epoch > n_epochs_prev_stages + training_schedule[stage]['n_epochs']:\n",
    "        n_epochs_prev_stages += training_schedule[stage]['n_epochs']\n",
    "        stage += 1\n",
    "    train_op = training_schedule[stage]['train_op']\n",
    "    learning_rate = training_schedule[stage]['lr']\n",
    "    for i, (tokens_batch, tags_batch, mask_batch, y_batch) in enumerate(dataset_iterator.gen_batches(batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=tag2idx)):\n",
    "        step += 1\n",
    "        losses_train_epoch = []\n",
    "#         print(tokens_batch)\n",
    "#         print(tags_batch)\n",
    "#         print(np.unique(y_batch))\n",
    "        feed = fill_feed_dict({tokens_input_ph: tokens_batch, mask_ph: mask_batch, y_ph: y_batch, learning_rate_ph: learning_rate})\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val1 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val1 = cell1_kernel.eval(session=sess)\n",
    "        # Train\n",
    "        with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "            loss_cur, _ = sess.run([loss, train_op], feed_dict=feed)\n",
    "#             print(np.unique(y_pred_cur))\n",
    "        losses_train_epoch.append(loss_cur)\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val2 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val2 = cell1_kernel.eval(session=sess)\n",
    "            d_cell0_kernel = np.linalg.norm(cell0_kernel_val2 - cell0_kernel_val1)/np.linalg.norm(cell0_kernel_val1)\n",
    "            d_cell1_kernel = np.linalg.norm(cell1_kernel_val2 - cell1_kernel_val1)/np.linalg.norm(cell1_kernel_val1)\n",
    "            d_elmo_cells_list['cell0'].append(d_cell0_kernel)\n",
    "            d_elmo_cells_list['cell1'].append(d_cell1_kernel)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r')\n",
    "            progress = i/n_batches_train\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(ceil(progress*20)), ceil(progress*100)))\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    losses_train.extend(losses_train_epoch)\n",
    "    losses_epoch['train'].append(np.mean(losses_train_epoch))\n",
    "    # Validate\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "#         loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "#         if use_crf:\n",
    "#           pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "#         else:\n",
    "#           pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "#     #         print(pred)\n",
    "#         pred = concatenate_arrays(pred)\n",
    "#     #         print(np.unique(pred))\n",
    "#     #         print(np.unique(y_valid_flat))\n",
    "#         f1_valid = f1_score(y_valid_flat, pred, average='macro')\n",
    "        res = eval_valid(dataset_iterator, tag2idx)\n",
    "        f1_valid = res['f1']\n",
    "        loss_valid = res['loss']\n",
    "        if isinstance(f1_valid, list) or isinstance(f1_valid, np.ndarray):\n",
    "            f1_valid = np.mean(f1_valid)\n",
    "        f1_scores['valid'].append(f1_valid)\n",
    "        if f1_valid > best_valid_f1:\n",
    "            best_valid_f1 = f1_valid\n",
    "    # Get elmo params\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        if TRAIN_ELMO:\n",
    "            layer_coeff, scale = sess.run([elmo_coef['layer_coefficients'], elmo_coef['scaling']])\n",
    "            if f1_valid == best_valid_f1:\n",
    "                elmo_params_best = {'layer_coefficients': layer_coeff, 'scaling': scale}\n",
    "    losses_epoch['valid'].append(loss_valid)\n",
    "    if epoch % display_epoch == 0 or epoch == 1:\n",
    "        print('Train loss = {}'.format(losses_epoch['train'][-1]))\n",
    "    #         print('Train F1 score = {}'.format(f1_scores['train'][-1]))\n",
    "        if TRAIN_ELMO:\n",
    "            with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "                print('ELMo weights:')\n",
    "                print('Coefficients = {}, scale = {}'.format(layer_coeff, scale))\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            print('ELMo cells change per epoch: cell0: {:.2f}%, cell1: {:.2f}%'.format(d_cell0_kernel*100, d_cell1_kernel*100))\n",
    "\n",
    "    if epoch % valid_epoch == 0 or epoch == 1:\n",
    "        print('Valid loss = {}'.format(losses_epoch['valid'][-1]))\n",
    "        print('Valid F1 score = {}'.format(f1_scores['valid'][-1]))\n",
    "n_steps = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1535747350783,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "DeYhVaYerGPA",
    "outputId": "a3c9c92a-7c1f-4745-852b-865372f7dffe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,n_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1535712909777,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "OI5WYrO9VP42",
    "outputId": "637c83b3-a6cb-4df7-84f5-8b7ea1270eca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [],\n",
       " 'valid': [array([0.99205847, 0.62723779, 0.        ]),\n",
       "  array([0.99253106, 0.62146051, 0.        ]),\n",
       "  array([0.99273815, 0.62730076, 0.        ]),\n",
       "  array([0.99276422, 0.62374395, 0.        ])]}"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1535712910868,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "V9ovIfbsanNR",
    "outputId": "eae24775-54c0-498c-ca92-f69c0391209f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5388360584161899"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1535712912900,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "szSGnmYdPaCA",
    "outputId": "e617cf0e-be52-4528-bc36-f3e420d48e23"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95b56567b8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "plt.plot(epochs, losses_epoch['train'], c='b', label='train')\n",
    "plt.plot(epochs, losses_epoch['valid'], c='r', label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.title('Learning curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1535712913958,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "JzWORmJ3PaCE",
    "outputId": "961e64ce-5dc4-4977-c189-c3341ca5334e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSn8gwqkCieQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95b5654f60>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot F1 scores\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "# plt.plot(steps, losses['train'], c='b', label='train')\n",
    "plt.plot(epochs, f1_scores['valid'], c='r', label='valid')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.title('F1 score history')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2v8VTscyPaCH"
   },
   "outputs": [],
   "source": [
    "elmo_vars_dict = {v.name: v for v in elmo_vars}\n",
    "saver_elmo = tf.train.Saver(elmo_vars_dict)\n",
    "save_path = saver_elmo.save(sess, \"/model_params_multiclass/elmo_multi.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)\n",
    "saver_all = tf.train.Saver()\n",
    "save_path = saver_all.save(sess, \"/model_params_multiclass/model_all_multi.ckpt\")\n",
    "print(\"Whole model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c6697CIGySa"
   },
   "outputs": [],
   "source": [
    "print('Best F1 score on validation: {:.2f}'.format(best_valid_f1*100))\n",
    "if TRAIN_ELMO:\n",
    "    layer_coeff, scale = sess.run([elmo_coef['layer_coefficients'], elmo_coef['scaling']])\n",
    "    elmo_params = {'layer_coefficients': layer_coeff, 'scaling': scale}\n",
    "    elmo_layer_coeff = np.exp(elmo_params['layer_coefficients'])*elmo_params['scaling']\n",
    "    print('Learned ELMo layer combination weights:')\n",
    "    print(elmo_layer_coeff)\n",
    "    print('Normalized:')\n",
    "    print(np.exp(elmo_layer_coeff)/np.sum(np.exp(elmo_layer_coeff)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "17. Finetune ELMo BIO-markup multiclass task.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
