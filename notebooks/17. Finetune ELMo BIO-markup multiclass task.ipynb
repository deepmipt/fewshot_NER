{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 19:06:59.820 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import *\n",
    "import copy\n",
    "import sys\n",
    "from deeppavlov.dataset_readers.ontonotes_reader import OntonotesReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ELMO = True\n",
    "TRAIN_ALL_ELMO_PARAMS = True\n",
    "MULTICLASS = True\n",
    "USE_BIO_MARKUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    reader = OntonotesReader()\n",
    "    dataset = reader.read(data_path='data/')\n",
    "    # print(dataset.keys())\n",
    "    print('Num of train sentences: {}'.format(len(dataset['train'])))\n",
    "    print('Num of valid sentences: {}'.format(len(dataset['valid'])))\n",
    "    print('Num of test sentences: {}'.format(len(dataset['test'])))\n",
    "    print(dataset['train'][50:60])\n",
    "    return dataset\n",
    "\n",
    "def filter_data_by_ne_type(data:list, ne_types:list, tags2binary=False, preserveBIO=False, keepIfAny=True):\n",
    "    if ne_types == None or len(ne_types) == 0:\n",
    "        return data\n",
    "    data_filtered = []\n",
    "    for tokens,tags in data:\n",
    "        contains_all = True\n",
    "        contains_any = False\n",
    "        tags_norm = [getNeTagMainPart(t) for t in tags]\n",
    "        for ne_type in ne_types:\n",
    "            if not ne_type in tags_norm:\n",
    "                contains_all = False\n",
    "            if ne_type in tags_norm:\n",
    "                contains_any = True\n",
    "        if contains_all or (keepIfAny and contains_any):\n",
    "            if tags2binary:\n",
    "                if preserveBIO:\n",
    "                    tags = [tags[i][:2]+'T' if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "                else:\n",
    "                    tags = ['T' if t in ne_types else 'O' for t in tags_norm]\n",
    "            else:\n",
    "                tags = [tags[i][:2]+t if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "            data_filtered.append((tokens,tags))\n",
    "    return data_filtered\n",
    "\n",
    "def filter_dataset_by_ne_types(dataset: list, ne_types, tags2binary=True, preserveBIO=False, keepIfAny=True):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    if not isinstance(ne_types, list):\n",
    "        ne_types = [ne_types]\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        dataset[dataset_type] = filter_data_by_ne_type(dataset[dataset_type], ne_types, tags2binary=tags2binary, preserveBIO=preserveBIO)\n",
    "        print('Num of {} sentences: {}'.format(dataset_type, len(dataset[dataset_type])))\n",
    "    return dataset\n",
    "\n",
    "def get_data_sample(data, n_samples: int):\n",
    "    indices = np.random.choice(len(data), size=n_samples, replace=False)\n",
    "    return split_tokens_tags([data[i] for i in indices])\n",
    "\n",
    "def get_tokens_len(tokens):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    return [len(seq) for seq in tokens]\n",
    "\n",
    "def to_lower_case(tokens:list):\n",
    "    tokens_lower = []\n",
    "    for seq in tokens:\n",
    "        tokens_lower.append([])\n",
    "        for token in seq:\n",
    "            tokens_lower[-1].append(token.lower())\n",
    "    return tokens_lower\n",
    "\n",
    "def add_padding(tokens:list):\n",
    "    if isinstance(tokens[0], str):\n",
    "        return tokens, len(tokens)\n",
    "    elif isinstance(tokens[0], list):\n",
    "        tokens = copy.deepcopy(tokens)\n",
    "        max_len = 0\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_len:\n",
    "                max_len = len(seq)\n",
    "        for seq in tokens:\n",
    "            i = len(seq)\n",
    "            while i < max_len:\n",
    "                seq.append('')\n",
    "                i += 1\n",
    "        return tokens\n",
    "    else:\n",
    "        raise Exception('tokens should be either list of strings or list of lists of strings')\n",
    "  \n",
    "def getNeTagMainPart(tag:str):\n",
    "    return tag[2:] if len(tag) > 2 else tag\n",
    "\n",
    "def tags2binaryFlat(tags):\n",
    "    return np.array([1 if t == 'T' or (len(t) > 2 and t[2:] == 'T') else 0 for seq in tags for t in seq])\n",
    "\n",
    "def tagsEncodePadded(tags:list, binary=True, tag2idx=None):\n",
    "    if tag2idx:\n",
    "        binary = False\n",
    "    if isinstance(tags[0], str):\n",
    "        tags = [tags]\n",
    "    n_sentences = len(tags)\n",
    "    tokens_length = get_tokens_len(tags)\n",
    "    max_len = np.max(tokens_length)\n",
    "    y = np.zeros((n_sentences, max_len))\n",
    "    for i, sen in enumerate(tags):\n",
    "        for j, tag in enumerate(sen):\n",
    "            if binary: \n",
    "                y[i][j] = 1 if tags[i][j] != 'O' else 0\n",
    "            else:\n",
    "                if tag2idx:\n",
    "                    tag_name = tag if USE_BIO_MARKUP else getNeTagMainPart(tag)\n",
    "                    y[i][j] = tag2idx[tag_name]\n",
    "                else:\n",
    "                    raise Exception('tag2idx dictionary should be provided')\n",
    "    return y\n",
    "\n",
    "def get_matrices(tokens, tags, embedder):\n",
    "    return (embeddings2feat_mat(embedder.embed(tokens), get_tokens_len(tokens)),\n",
    "           tags2binaryFlat(tags))\n",
    "  \n",
    "def split_tokens_tags(dataset: list):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for sample in dataset:\n",
    "        tokens.append(sample[0])\n",
    "        tags.append(sample[1])\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train sentences: 75187\n",
      "Num of valid sentences: 9603\n",
      "Num of test sentences: 9479\n",
      "[(['Actions', 'had', 'to', 'be', 'taken', 'to', 'break', 'through', 'the', 'blockade', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'a', 'night', 'in', 'late', 'July', '1940', ',', 'the', 'atmosphere', 'in', 'Zhuanbi', 'Village', 'in', 'Shaanxi', 'was', 'unusual', '.'], ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'B-GPE', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'that', 'dark', 'night', ',', 'everyone', 'was', 'sleeping', 'when', 'human', 'voices', 'and', 'neighing', 'horses', 'were', 'heard', 'within', 'the', 'village', '.'], ['O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['People', 'all', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Did', 'something', 'happen', '?'], ['O', 'O', 'O', 'O']), (['Some', 'folks', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Opening', 'the', 'street', 'gate', ',', 'they', 'saw', 'a', 'soldier', 'standing', 'by', 'the', 'gate', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Folks', ',', 'go', 'back', ',', 'go', 'back', ',', 'nothing', 'is', 'wrong', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Our', 'troops', 'are', 'just', 'going', 'to', 'stay', 'here', 'for', 'the', 'night', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n",
      "NE types used for training:\n",
      "['PERSON']\n",
      "1 in total\n",
      "Num of train sentences: 12195\n",
      "Num of valid sentences: 1553\n",
      "Num of test sentences: 1573\n",
      "Number of sentences in filtered dataset: train: 12195, valid: 1553, test: 1573\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = read_data()\n",
    "ne_types_holdout = ['PERSON']\n",
    "ne_types_all = ['GPE','DATE','ORG','EVENT','LOC','FAC','CARDINAL','QUANTITY','NORP','ORDINAL','WORK_OF_ART', 'LANGUAGE', 'TIME', 'PRODUCT', 'MONEY', 'LAW', 'PERCENT', 'PERSON']\n",
    "ne_types = [t for t in ne_types_all if t not in ne_types_holdout]\n",
    "ne_types = ['PERSON']\n",
    "print('NE types used for training:')\n",
    "print(ne_types)\n",
    "print('{} in total'.format(len(ne_types)))\n",
    "dataset = filter_dataset_by_ne_types(dataset_orig, ne_types, tags2binary=False, keepIfAny=True)\n",
    "print('Number of sentences in filtered dataset: train: {}, valid: {}, test: {}'.format(len(dataset['train']), len(dataset['valid']), len(dataset['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hayao', 'Tada', ',', 'commander', 'of', 'the', 'Japanese', 'North', 'China', 'Area', 'Army', ',', 'adopted', 'a', 'strategy', 'of', 'siege', 'warfare', 'to', 'deal', 'with', 'the', 'Eighth', 'Route', 'Army', '.'], ['B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['In', 'December', '1939', ',', 'Commander', '-', 'in', '-', 'chief', 'Zhu', 'De', 'and', 'Vice', 'Commander', 'Peng', 'Dehuai', 'of', 'the', 'Eighth', 'Route', 'Army', 'received', 'a', 'top', '-', 'secret', 'telegram', 'from', 'Commander', 'Lu', 'Zhengcao', 'of', 'the', 'Jizhong', 'Military', 'District', ',', 'among', 'other', 'people', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['This', 'plot', 'of', 'the', 'Japanese', 'army', 'drew', 'great', 'attention', 'from', 'Zhu', 'De', 'and', 'Peng', 'Dehuai', 'of', 'Eighth', 'Route', 'Army', 'headquarters', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'July', '22', ',', '1940', ',', 'a', 'campaign', 'preparation', 'order', 'to', 'attack', 'the', 'Zhengtai', 'Railway', ',', 'jointly', 'signed', 'by', 'Zhu', 'De', ',', 'Peng', 'Dehuai', ',', 'and', 'Zuo', 'Quan', ',', 'was', 'sent', 'to', \"Yan'an\", 'and', 'all', 'units', 'of', 'the', 'Eighth', 'Route', 'Army', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['The', 'head', 'of', 'the', 'County', 'People', \"'s\", 'Armed', 'Forces', 'Commission', 'was', 'named', 'Zhao', 'Yemin', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']), (['Zhao', 'Yemin', 'came', ',', 'ah', ',', 'bringing', 'a', 'group', 'of', 'armed', 'personnel', 'and', 'some', 'things', '.'], ['B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Led', 'by', 'Nie', 'Rongzhen', ',', 'the', 'Shanxi', '-', 'Chahar', '-', 'Hebei', 'Military', 'District', 'was', 'ready', 'for', 'an', 'offensive', ',', 'while', 'at', 'the', 'same', 'time', ',', 'fifteen', 'regiments', 'laid', 'an', 'ambush', 'in', 'advance', 'along', 'the', 'Zhengtai', 'Railway', ',', 'launching', 'fierce', 'attacks', 'on', 'Japanese', 'military', 'strongholds', 'along', 'the', 'railway', 'line', '.'], ['O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['It', 'was', 'also', 'during', 'the', 'night', 'of', 'August', '20', 'that', 'the', '129th', 'Division', 'directed', 'by', 'Division', 'Commander', 'Liu', 'Bocheng', 'and', 'Political', 'Commissar', 'Deng', 'Xiaoping', ',', 'with', 'eight', 'regiments', ',', 'including', 'two', 'regiments', 'of', 'the', '1st', 'Dare', '-', 'to', '-', 'Die', 'Corps', 'and', 'eight', 'independent', 'battalions', ',', 'formed', 'left', '-', 'flank', 'and', 'right', '-', 'flank', 'commandos', 'and', 'a', 'central', 'contingent', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['It', 'was', 'still', 'during', 'this', 'night', 'that', 'the', '120th', 'Division', ',', 'directed', 'by', 'Division', 'Commander', 'He', 'Long', 'and', 'Political', 'Commissar', 'Guan', 'Xiangying', ',', 'with', '20', 'regiments', ',', 'sabotaged', 'the', 'northern', 'section', 'of', 'the', 'Tongpu', 'Railway', 'and', 'some', 'main', 'roads', 'west', 'of', 'the', 'railway', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PERSON': 1, 'I-PERSON': 2}\n",
      "['O', 'B-PERSON', 'I-PERSON']\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {}\n",
    "idx2tag = []\n",
    "tag2idx['O'] = 0\n",
    "idx2tag.append('O')\n",
    "i = 1\n",
    "for tag in ne_types:\n",
    "    if USE_BIO_MARKUP:\n",
    "        tag2idx['B-'+tag] = i\n",
    "        tag2idx['I-'+tag] = i+1\n",
    "        idx2tag.append('B-'+tag)\n",
    "        idx2tag.append('I-'+tag)\n",
    "        i += 2\n",
    "    else:\n",
    "        tag2idx[tag] = i\n",
    "        idx2tag.append(tag)\n",
    "        i += 1\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.common.check_gpu import GPU_AVAILABLE\n",
    "INITIALIZER = tf.contrib.layers.xavier_initializer\n",
    "\n",
    "def cudnn_lstm_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "                       initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "\n",
    "    if GPU_AVAILABLE:\n",
    "        return cudnn_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                          seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "    log.info('\\nWarning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. '\n",
    "             'It is okay for inference mode, but '\n",
    "             'if you train your model with this cell it could NOT be used with '\n",
    "             'tf.contrib.cudnn_rnn.CudnnLSTMCell later. '\n",
    "             )\n",
    "\n",
    "    return cudnn_compatible_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                                 seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "def cudnn_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "               initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "    \"\"\" Fast CuDNN LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            seq_lengths: tensor of sequence lengths with dimension [B]\n",
    "            initial_h: optional initial hidden state, masks trainable_initial_states\n",
    "                if provided\n",
    "            initial_c: optional initial cell state, masks trainable_initial_states\n",
    "                if provided\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=n_layers,\n",
    "                                              num_units=n_hidden)\n",
    "        if trainable_initial_states:\n",
    "            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n",
    "            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n",
    "            init_c = tf.get_variable('init_c', [n_layers, 1, n_hidden])\n",
    "            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n",
    "        else:\n",
    "            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n",
    "\n",
    "        initial_h = initial_h or init_h\n",
    "        initial_c = initial_c or init_c\n",
    "\n",
    "        h, (h_last, c_last) = lstm(tf.transpose(units, (1, 0, 2)), (initial_h, initial_c))\n",
    "        h = tf.transpose(h, (1, 0, 2))\n",
    "        h_last = tf.reshape(h_last, shape=(-1, n_hidden))\n",
    "        c_last = tf.reshape(c_last, shape=(-1, n_hidden))\n",
    "\n",
    "        # Extract last states if they are provided\n",
    "        if seq_lengths is not None:\n",
    "            indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1)\n",
    "            h_last = tf.gather_nd(h, indices)\n",
    "\n",
    "        return h, (h_last, c_last)\n",
    "\n",
    "def cudnn_bi_lstm(units,\n",
    "                  n_hidden,\n",
    "                  seq_lengths=None,\n",
    "                  n_layers=1,\n",
    "                  trainable_initial_states=False,\n",
    "                  name='cudnn_bi_lstm',\n",
    "                  reuse=False):\n",
    "    \"\"\" Fast CuDNN Bi-LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            seq_lengths: number of tokens in each sample in the batch\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        if seq_lengths is None:\n",
    "            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n",
    "        with tf.variable_scope('Forward'):\n",
    "            h_fw, (h_fw_last, c_fw_last) = cudnn_lstm_wrapper(units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "        with tf.variable_scope('Backward'):\n",
    "            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "            h_bw, (h_bw_last, c_bw_last) = cudnn_lstm_wrapper(reversed_units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "        return (h_fw, h_bw), ((h_fw_last, c_fw_last), (h_bw_last, c_bw_last))\n",
    "\n",
    "def bi_rnn(units: tf.Tensor,\n",
    "           n_hidden: List,\n",
    "           cell_type='gru',\n",
    "           seq_lengths=None,\n",
    "           trainable_initial_states=False,\n",
    "           use_peepholes=False,\n",
    "           name='Bi-'):\n",
    "    \"\"\" Bi directional recurrent neural network. GRU or LSTM\n",
    "\n",
    "        Args:\n",
    "            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n",
    "            n_hidden_list: list with number of hidden units at the ouput of each layer\n",
    "            seq_lengths: length of sequences for different length sequences in batch\n",
    "                can be None for maximum length as a length for every sample in the batch\n",
    "            cell_type: 'lstm' or 'gru'\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            use_peepholes: whether to use peephole connections (only 'lstm' case affected)\n",
    "            name: what variable_scope to use for the network parameters\n",
    "            add_l2_losses: whether to add l2 losses on network kernels to\n",
    "                tf.GraphKeys.REGULARIZATION_LOSSES or not\n",
    "        Returns:\n",
    "            units: tensor at the output of the last recurrent layer\n",
    "                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n",
    "            last_units: tensor of last hidden states for GRU and tuple\n",
    "                of last hidden stated and last cell states for LSTM\n",
    "                dimensionality of cell states and hidden states are\n",
    "                similar and equal to [B x 2 * H], where B - batch\n",
    "                size and H is number of hidden units\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(name + '_' + cell_type.upper()):\n",
    "        if cell_type == 'gru':\n",
    "            forward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "                initial_state_bw = tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        elif cell_type == 'lstm':\n",
    "            forward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_fw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "                initial_state_bw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_bw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        else:\n",
    "            raise RuntimeError('cell_type must be either \"gru\" or \"lstm\"s')\n",
    "        (rnn_output_fw, rnn_output_bw), (fw, bw) = \\\n",
    "            tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                            backward_cell,\n",
    "                                            units,\n",
    "                                            dtype=tf.float32,\n",
    "                                            sequence_length=seq_lengths,\n",
    "                                            initial_state_fw=initial_state_fw,\n",
    "                                            initial_state_bw=initial_state_bw)\n",
    "    kernels = [var for var in forward_cell.trainable_variables +\n",
    "               backward_cell.trainable_variables if 'kernel' in var.name]\n",
    "    for kernel in kernels:\n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(kernel))\n",
    "    return (rnn_output_fw, rnn_output_bw), (fw, bw)\n",
    "\n",
    "def build_cudnn_rnn(units, mask, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        with tf.variable_scope(cell_type.upper() + '_' + str(n)):\n",
    "            if cell_type.lower() == 'lstm':\n",
    "                units, _ = cudnn_bi_lstm(units, n_hidden, sequence_lengths)\n",
    "            elif cell_type.lower() == 'gru':\n",
    "                units, _ = cudnn_bi_gru(units, n_hidden, sequence_lengths)\n",
    "            else:\n",
    "                raise RuntimeError('Wrong cell type \"{}\"! Only \"gru\" and \"lstm\"!'.format(cell_type))\n",
    "            units = tf.concat(units, -1)\n",
    "            if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "                units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_rnn(units, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        units, _ = bi_rnn(units, n_hidden, cell_type=cell_type, name='Layer_' + str(n))\n",
    "        units = tf.concat(units, -1)\n",
    "        if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "            units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_top(units, n_tags=18*2+1, top_dropout:bool=False, two_dense_on_top:bool=False, n_hidden=128):\n",
    "    if top_dropout:\n",
    "        units = variational_dropout(units, dropout_ph)\n",
    "    if two_dense_on_top:\n",
    "        units = tf.layers.dense(units, n_hidden, activation=tf.nn.relu,\n",
    "                                kernel_initializer=INITIALIZER(),\n",
    "                                kernel_regularizer=tf.nn.l2_loss)\n",
    "    logits = tf.layers.dense(units, n_tags, activation=None,\n",
    "                             kernel_initializer=INITIALIZER(),\n",
    "                             kernel_regularizer=tf.nn.l2_loss)\n",
    "    return logits\n",
    "\n",
    "def build_train_predict(logits, n_tags, mask, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg):\n",
    "    res = {}\n",
    "    if use_crf:\n",
    "        sequence_lengths = tf.reduce_sum(mask, axis=1)\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(logits, y_ph, sequence_lengths)\n",
    "        loss_tensor = -log_likelihood\n",
    "        res['transition_params'] = transition_params\n",
    "    else:\n",
    "        ground_truth_labels = tf.one_hot(y_ph, n_tags)\n",
    "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_labels, logits=logits)\n",
    "        loss_tensor = loss_tensor * mask\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        res['y_pred'] = y_pred\n",
    "\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "\n",
    "    # L2 regularization\n",
    "    if l2_reg > 0:\n",
    "        loss += l2_reg * tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    res['loss'] = loss\n",
    "        \n",
    "    # optimizer = partial(tf.train.MomentumOptimizer, momentum=0.9, use_nesterov=True)\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    train_op = get_train_op(loss, learning_rate_ph, optimizer, clip_norm=clip_grad_norm)\n",
    "    res['train_op'] = train_op\n",
    "    return res\n",
    "\n",
    "def predict_no_crf(y_pred, mask_ph, feed_dict):\n",
    "    pred_idxs, mask = sess.run([y_pred, mask_ph], feed_dict)\n",
    "\n",
    "    # Filter by sequece length\n",
    "    sequence_lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "    pred = []\n",
    "    for utt, l in zip(pred_idxs, sequence_lengths):\n",
    "        pred.append(utt[:l])\n",
    "    return pred\n",
    "\n",
    "def predict_crf(logits, transition_params, mask_ph, feed_dict):\n",
    "    logits, trans_params, mask = sess.run([logits,\n",
    "                                           transition_params,\n",
    "                                           mask_ph],\n",
    "                                           feed_dict=feed_dict)\n",
    "    sequence_lengths = np.maximum(np.sum(mask, axis=1).astype(np.int32), 1)\n",
    "    # iterate over the sentences because no batching in viterbi_decode\n",
    "    pred = []\n",
    "    for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "        logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
    "        viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "        pred += [viterbi_seq]\n",
    "    return pred\n",
    "\n",
    "def get_train_op(loss,\n",
    "                 learning_rate,\n",
    "                 optimizer=None,\n",
    "                 clip_norm=None,\n",
    "                 learnable_scopes=None,\n",
    "                 optimizer_scope_name=None,\n",
    "                 trainable_vars:list=None):\n",
    "    \"\"\" Get train operation for given loss\n",
    "\n",
    "    Args:\n",
    "        loss: loss, tf tensor or scalar\n",
    "        learning_rate: scalar or placeholder\n",
    "        clip_norm: clip gradients norm by clip_norm\n",
    "        learnable_scopes: which scopes are trainable (None for all)\n",
    "        optimizer: instance of tf.train.Optimizer, default Adam\n",
    "\n",
    "    Returns:\n",
    "        train_op\n",
    "    \"\"\"\n",
    "    if optimizer_scope_name is None:\n",
    "        opt_scope = tf.variable_scope('Optimizer')\n",
    "    else:\n",
    "        opt_scope = tf.variable_scope(optimizer_scope_name)\n",
    "    with opt_scope:\n",
    "        if learnable_scopes is None:\n",
    "            variables_to_train = tf.global_variables()\n",
    "        else:\n",
    "            variables_to_train = []\n",
    "            for scope_name in learnable_scopes:\n",
    "                for var in tf.global_variables():\n",
    "                    if scope_name in var.name:\n",
    "                        variables_to_train.append(var)\n",
    "        if trainable_vars:\n",
    "            variables_to_train = trainable_vars\n",
    "            \n",
    "        if optimizer is None:\n",
    "            optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "        # For batch norm it is necessary to update running averages\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            opt = optimizer(learning_rate)\n",
    "            grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n",
    "            if clip_norm is not None:\n",
    "                grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                  for grad, var in grads_and_vars] #  if grad is not None\n",
    "            train_op = opt.apply_gradients(grads_and_vars)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(prob: np.ndarray, threshold=0.5):\n",
    "    labels = np.zeros(prob.shape)\n",
    "    labels[prob > threshold] = 1\n",
    "    return labels\n",
    "def flat_array(a: np.ndarray):\n",
    "#     return np.reshape(a, a.size)\n",
    "    return a.flatten()\n",
    "def calc_f1(y, pred_prob, average='binary'):\n",
    "    return f1_score(flat_array(y), flat_array(predict_labels(pred_prob)), average=average)\n",
    "def get_batch(dataset, batch_size=None, binaryTags=True, tag2idx=None):\n",
    "    if not batch_size:\n",
    "        batch_size = len(dataset)\n",
    "    tokens, tags = get_data_sample(dataset, batch_size)\n",
    "    mask = make_mask(tokens)\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    tokens = add_padding(tokens)\n",
    "    y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "    return tokens, tags, mask, y\n",
    "\n",
    "def make_mask(seq_list):\n",
    "  seq_count = len(seq_list)\n",
    "  seq_length = [len(s) for s in seq_list]\n",
    "  max_len = np.max(seq_length)\n",
    "  mask = np.zeros((seq_count, max_len), dtype=int)\n",
    "  seq_length = np.tile(np.expand_dims(seq_length, axis=-1), (1, max_len))\n",
    "  range_ar = np.tile(np.arange(1, max_len+1, 1), (seq_count, 1))\n",
    "  mask[range_ar <= seq_length] = 1\n",
    "  return mask\n",
    "\n",
    "def flatten_with_mask(seq_mat, mask):\n",
    "  return seq_mat[mask == 1]\n",
    "\n",
    "def concatenate_arrays(ar_list):\n",
    "  return np.concatenate(ar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, data):\n",
    "        self.data = {\n",
    "            'train': data['train'],\n",
    "            'valid': data['valid'],\n",
    "            'test': data['test']\n",
    "        }\n",
    "        \n",
    "    def get_samples_count(self, data_type='train'):\n",
    "        return len(self.data[data_type])\n",
    "\n",
    "    def gen_batches(self, batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=None):\n",
    "        indices = np.arange(len(self.data[data_type]))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        n = indices.size\n",
    "        k = 0\n",
    "        while k < n:\n",
    "            top = k + batch_size\n",
    "            if top > n: \n",
    "                top = n\n",
    "            indices_batch = indices[k:top]\n",
    "            tokens,tags = split_tokens_tags([self.data[data_type][i] for i in indices_batch])\n",
    "            mask = make_mask(tokens)\n",
    "            tokens_length = get_tokens_len(tokens)\n",
    "            tokens = add_padding(tokens)\n",
    "            y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "            yield tokens, tags, mask, y\n",
    "            k += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_valid(batch_generator, tag2idx):\n",
    "    pred_total = []\n",
    "    true_total = []\n",
    "    loss_valid_total = 0\n",
    "    for i, (tokens_valid, tags_valid, mask_valid, y_valid) in enumerate(batch_generator.gen_batches(32, data_type='valid', shuffle=True, binaryTags=False, tag2idx=tag2idx)):\n",
    "        feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "        y_valid_flat = flatten_with_mask(y_valid, mask_valid)\n",
    "        loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "        if use_crf:\n",
    "            pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "        else:\n",
    "            pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "        pred = concatenate_arrays(pred)\n",
    "        loss_valid_total += loss_valid\n",
    "        pred_total = np.concatenate([pred_total, pred])\n",
    "        true_total = np.concatenate([true_total, y_valid_flat])\n",
    "    f1_valid = calc_f1(true_total, pred_total, average='macro')\n",
    "    loss_valid_total = loss_valid_total/(i+1)\n",
    "    \n",
    "    return {'loss': loss_valid_total, 'f1': f1_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 19:07:05.71 INFO in 'tensorflow'['tf_logging'] at line 159: Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=TRAIN_ELMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0' shape=(1024, 16384) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0' shape=(16384,) dtype=float32>, <tf.Variable 'module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0' shape=(4096, 512) dtype=float32>, <tf.Variable 'module/aggregation/weights:0' shape=(3,) dtype=float32>, <tf.Variable 'module/aggregation/scaling:0' shape=() dtype=float32>]\n",
      "{'layer_coefficients': <tf.Variable 'module/aggregation/weights:0' shape=(3,) dtype=float32>, 'scaling': <tf.Variable 'module/aggregation/scaling:0' shape=() dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "print(tf.trainable_variables())\n",
    "if(TRAIN_ELMO):\n",
    "    elmo_coef = {'layer_coefficients': tf.trainable_variables()[-2], 'scaling': tf.trainable_variables()[-1]}\n",
    "    print(elmo_coef)\n",
    "elmo_vars = tf.trainable_variables()\n",
    "elmo_vars_coef = list(elmo_coef.values())\n",
    "elmo_vars_cell_weights = [v for v in elmo_vars if v not in elmo_vars_coef]\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}\n",
    "if TRAIN_ALL_ELMO_PARAMS:\n",
    "    cell0_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0']\n",
    "    cell1_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "use_cudnn_rnn = True\n",
    "l2_reg = 0\n",
    "n_hidden_list = (128,)\n",
    "cell_type = 'lstm'\n",
    "n_tags = len(idx2tag)\n",
    "use_crf = True\n",
    "clip_grad_norm = 5.0\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "tokens_input_ph = tf.placeholder(shape=[None, None], dtype=tf.string)\n",
    "# tokens_length_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "y_ph = tf.placeholder(shape=[None, None], dtype=tf.int32, name='y_ph')\n",
    "learning_rate_ph = tf.placeholder_with_default(learning_rate, shape=[], name='learning_rate')\n",
    "dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name='dropout')\n",
    "training_ph = tf.placeholder_with_default(False, shape=[], name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(inp: dict, train=True):\n",
    "    feed_dict = {learning_rate_ph: learning_rate, dropout_ph: dropout_keep_prob if train else 1.0, training_ph: train}\n",
    "    feed_dict.update(inp)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 19:07:05.720 INFO in 'tensorflow'['tf_logging'] at line 115: Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-43f176f69ae1>:117: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 19:07:05.850 WARNING in 'tensorflow'['tf_logging'] at line 125: From <ipython-input-7-43f176f69ae1>:117: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/clement/virtenv/env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 19:07:05.876 WARNING in 'tensorflow'['tf_logging'] at line 125: From /home/clement/virtenv/env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "/home/clement/virtenv/env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "y_pred = None\n",
    "transition_params = None\n",
    "tokens_length = tf.to_int32(tf.reduce_sum(mask_ph, axis=1))\n",
    "emb = elmo(inputs={\"tokens\": tokens_input_ph,\n",
    "                    \"sequence_len\": tokens_length},\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)['elmo']\n",
    "# mask = tf.sequence_mask(lengths=tokens_length_ph, dtype=tf.float32)\n",
    "features = emb\n",
    "if use_cudnn_rnn:\n",
    "    units = build_cudnn_rnn(features, mask_ph, n_hidden_list, cell_type)\n",
    "else:\n",
    "    units = build_rnn(features, n_hidden_list, cell_type)\n",
    "\n",
    "logits = build_top(units, n_tags=n_tags)\n",
    "\n",
    "out_dict = build_train_predict(logits, n_tags, mask_ph, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg)\n",
    "train_op_all = out_dict['train_op']\n",
    "loss = out_dict['loss']\n",
    "if use_crf:\n",
    "    transition_params = out_dict['transition_params']\n",
    "else:\n",
    "    y_pred = out_dict['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'LSTM_0/cudnn_bi_lstm/Forward/cudnn_lstm/cudnn_lstm/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'LSTM_0/cudnn_bi_lstm/Backward/cudnn_lstm/cudnn_lstm/opaque_kernel:0' shape=<unknown> dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(256, 3) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'transitions:0' shape=(3, 3) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "model_vars = [v for v in all_vars if v not in elmo_vars]\n",
    "print(model_vars)\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/virtenv/env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Optimizers for different parameters\n",
    "with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE):\n",
    "    train_op_model = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=model_vars)\n",
    "    train_op_elmo = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars)\n",
    "    train_op_elmo_coef = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_coef)\n",
    "    train_op_elmo_cell_weights = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_cell_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('./graph/bilstm_crf_elmo_bio_multi', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlqOMRZ_kVuf"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run([initialize_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = DatasetIterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sen_size = 100\n",
    "tokens_valid, tags_valid, mask_valid, y_valid = get_batch(dataset['valid'], valid_sen_size, binaryTags=False, tag2idx=tag2idx)\n",
    "feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "y_valid_flat = flatten_with_mask(y_valid, mask_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tags_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_schedule = [{'train_op': train_op_model, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_cell_weights, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_coef, 'n_epochs': 200, 'lr': 1e-2}]\n",
    "training_schedule = [{'train_op': train_op_model, 'n_epochs': 4, 'lr': 3e-3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "[====================] 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/virtenv/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.15954463183879852\n",
      "ELMo weights:\n",
      "Coefficients = [0. 0. 0.], scale = 1.0\n",
      "ELMo cells change per epoch: cell0: 0.00%, cell1: 0.00%\n",
      "Valid loss = 0.35011279491745695\n",
      "Valid F1 score = 0.5739034133736736\n",
      "Epoch 2/4\n",
      "[====================] 100%\n",
      "Train loss = 0.04954783245921135\n",
      "ELMo weights:\n",
      "Coefficients = [0. 0. 0.], scale = 1.0\n",
      "ELMo cells change per epoch: cell0: 0.00%, cell1: 0.00%\n",
      "Valid loss = 0.3282947147987327\n",
      "Valid F1 score = 0.5743973891855031\n",
      "Epoch 3/4\n",
      "[=========           ] 44%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f21e1c116eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mlosses_train_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTRAIN_ALL_ELMO_PARAMS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtenv/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = np.sum([s['n_epochs'] for s in training_schedule])\n",
    "stage = 0\n",
    "n_epochs_prev_stages = 0\n",
    "display_epoch = 1\n",
    "valid_epoch = 1\n",
    "losses_train = []\n",
    "losses_epoch = {'train': [], 'valid': []}\n",
    "f1_scores = {'train': [], 'valid': []}\n",
    "best_valid_f1 = 0\n",
    "d_elmo_cells_list = {'cell0':[], 'cell1':[]}\n",
    "n_batches_train = ceil(len(dataset['train'])/batch_size)\n",
    "step = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "    if epoch > n_epochs_prev_stages + training_schedule[stage]['n_epochs']:\n",
    "        n_epochs_prev_stages += training_schedule[stage]['n_epochs']\n",
    "        stage += 1\n",
    "    train_op = training_schedule[stage]['train_op']\n",
    "    learning_rate = training_schedule[stage]['lr']\n",
    "    for i, (tokens_batch, tags_batch, mask_batch, y_batch) in enumerate(dataset_iterator.gen_batches(batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=tag2idx)):\n",
    "        step += 1\n",
    "        losses_train_epoch = []\n",
    "#         print(tokens_batch)\n",
    "#         print(tags_batch)\n",
    "        feed = fill_feed_dict({tokens_input_ph: tokens_batch, mask_ph: mask_batch, y_ph: y_batch, learning_rate_ph: learning_rate})\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val1 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val1 = cell1_kernel.eval(session=sess)\n",
    "        # Train\n",
    "        with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "            loss_cur, _ = sess.run([loss, train_op], feed_dict=feed)\n",
    "        losses_train_epoch.append(loss_cur)\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val2 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val2 = cell1_kernel.eval(session=sess)\n",
    "            d_cell0_kernel = np.linalg.norm(cell0_kernel_val2 - cell0_kernel_val1)/np.linalg.norm(cell0_kernel_val1)\n",
    "            d_cell1_kernel = np.linalg.norm(cell1_kernel_val2 - cell1_kernel_val1)/np.linalg.norm(cell1_kernel_val1)\n",
    "            d_elmo_cells_list['cell0'].append(d_cell0_kernel)\n",
    "            d_elmo_cells_list['cell1'].append(d_cell1_kernel)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r')\n",
    "            progress = i/n_batches_train\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(ceil(progress*20)), ceil(progress*100)))\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    losses_train.extend(losses_train_epoch)\n",
    "    losses_epoch['train'].append(np.mean(losses_train_epoch))\n",
    "    # Validate\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "#         loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "#         if use_crf:\n",
    "#           pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "#         else:\n",
    "#           pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "#     #         print(pred)\n",
    "#         pred = concatenate_arrays(pred)\n",
    "#     #         print(np.unique(pred))\n",
    "#     #         print(np.unique(y_valid_flat))\n",
    "#         f1_valid = calc_f1(y_valid_flat, pred, average='macro')\n",
    "        res = eval_valid(dataset_iterator, tag2idx)\n",
    "        f1_valid = res['f1']\n",
    "        loss_valid = res['loss']\n",
    "        f1_scores['valid'].append(f1_valid)\n",
    "        if f1_valid > best_valid_f1:\n",
    "                best_valid_f1 = f1_valid\n",
    "    # Get elmo params\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        if TRAIN_ELMO:\n",
    "            layer_coeff, scale = sess.run([elmo_coef['layer_coefficients'], elmo_coef['scaling']])\n",
    "            if f1_valid == best_valid_f1:\n",
    "                elmo_params_best = {'layer_coefficients': layer_coeff, 'scaling': scale}\n",
    "    losses_epoch['valid'].append(loss_valid)\n",
    "    if epoch % display_epoch == 0 or epoch == 1:\n",
    "        print('Train loss = {}'.format(losses_epoch['train'][-1]))\n",
    "    #         print('Train F1 score = {}'.format(f1_scores['train'][-1]))\n",
    "        if TRAIN_ELMO:\n",
    "            with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "                print('ELMo weights:')\n",
    "                print('Coefficients = {}, scale = {}'.format(layer_coeff, scale))\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            print('ELMo cells change per epoch: cell0: {:.2f}%, cell1: {:.2f}%'.format(d_cell0_kernel*100, d_cell1_kernel*100))\n",
    "\n",
    "    if epoch % valid_epoch == 0 or epoch == 1:\n",
    "        print('Valid loss = {}'.format(losses_epoch['valid'][-1]))\n",
    "        print('Valid F1 score = {}'.format(f1_scores['valid'][-1]))\n",
    "n_steps = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "plt.plot(epochs, losses_epoch['train'], c='b', label='train')\n",
    "plt.plot(epochs, losses_epoch['valid'], c='r', label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.title('Learning curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 scores\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "# plt.plot(steps, losses['train'], c='b', label='train')\n",
    "plt.plot(epochs, f1_scores['valid'], c='r', label='valid')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.title('F1 score history')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
