{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5484,
     "status": "ok",
     "timestamp": 1535754876249,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "8FklJFwcPj4U",
    "outputId": "5b85dbb6-4bf1-456e-bc79-b2757286463c"
   },
   "outputs": [],
   "source": [
    "# For google colab\n",
    "# ! pip install deeppavlov\n",
    "# ! pip install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWrowqeyPaAj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostrovskiy/prog/AI/NLP/DeepPavlov/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kostrovskiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kostrovskiy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/kostrovskiy/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/kostrovskiy/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2018-09-07 15:58:36.361 DEBUG in 'gensim.models.doc2vec'['doc2vec'] at line 73: Fast version of gensim.models.doc2vec is being used\n",
      "2018-09-07 15:58:36.368 INFO in 'summa.preprocessing.cleaner'['textcleaner'] at line 20: 'pattern' package not found; tag filters are not available for English\n",
      "2018-09-07 15:58:44.379 DEBUG in 'matplotlib.backends'['__init__'] at line 90: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import *\n",
    "import copy\n",
    "import sys\n",
    "from deeppavlov.dataset_readers.ontonotes_reader import OntonotesReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz9piiTQa_Nr"
   },
   "outputs": [],
   "source": [
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGoiwLqLPaAo"
   },
   "outputs": [],
   "source": [
    "TRAIN_ELMO = True\n",
    "TRAIN_ALL_ELMO_PARAMS = True\n",
    "MULTICLASS = True\n",
    "USE_BIO_MARKUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QD4qUfNcPaAr"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    reader = OntonotesReader()\n",
    "    dataset = reader.read(data_path='data/')\n",
    "    # print(dataset.keys())\n",
    "    print('Num of train sentences: {}'.format(len(dataset['train'])))\n",
    "    print('Num of valid sentences: {}'.format(len(dataset['valid'])))\n",
    "    print('Num of test sentences: {}'.format(len(dataset['test'])))\n",
    "    print(dataset['train'][50:60])\n",
    "    return dataset\n",
    "\n",
    "def filter_data_by_ne_type(data:list, ne_types:list, tags2binary=False, preserveBIO=False, keepIfAny=True):\n",
    "    if ne_types == None or len(ne_types) == 0:\n",
    "        return data\n",
    "    data_filtered = []\n",
    "    for tokens,tags in data:\n",
    "        contains_all = True\n",
    "        contains_any = False\n",
    "        tags_norm = [getNeTagMainPart(t) for t in tags]\n",
    "        for ne_type in ne_types:\n",
    "            if not ne_type in tags_norm:\n",
    "                contains_all = False\n",
    "            if ne_type in tags_norm:\n",
    "                contains_any = True\n",
    "        if contains_all or (keepIfAny and contains_any):\n",
    "            if tags2binary:\n",
    "                if preserveBIO:\n",
    "                    tags = [tags[i][:2]+'T' if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "                else:\n",
    "                    tags = ['T' if t in ne_types else 'O' for t in tags_norm]\n",
    "            else:\n",
    "                if preserveBIO:\n",
    "                    tags = [tags[i][:2]+t if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "                else:\n",
    "                    tags = [t if t in ne_types else 'O' for i,t in enumerate(tags_norm)]\n",
    "            data_filtered.append((tokens,tags))\n",
    "    return data_filtered\n",
    "\n",
    "def filter_dataset_by_ne_types(dataset: list, ne_types, tags2binary=True, preserveBIO=False, keepIfAny=True):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    if not isinstance(ne_types, list):\n",
    "        ne_types = [ne_types]\n",
    "    for dataset_type in ['train', 'valid', 'test']:\n",
    "        dataset[dataset_type] = filter_data_by_ne_type(dataset[dataset_type], ne_types, tags2binary=tags2binary, preserveBIO=preserveBIO)\n",
    "        print('Num of {} sentences: {}'.format(dataset_type, len(dataset[dataset_type])))\n",
    "    return dataset\n",
    "\n",
    "def get_data_sample(data, n_samples: int):\n",
    "    indices = np.random.choice(len(data), size=n_samples, replace=False)\n",
    "    return split_tokens_tags([data[i] for i in indices])\n",
    "\n",
    "def get_tokens_len(tokens):\n",
    "    if isinstance(tokens[0], str):\n",
    "        tokens = [tokens]\n",
    "    return [len(seq) for seq in tokens]\n",
    "\n",
    "def to_lower_case(tokens:list):\n",
    "    tokens_lower = []\n",
    "    for seq in tokens:\n",
    "        tokens_lower.append([])\n",
    "        for token in seq:\n",
    "            tokens_lower[-1].append(token.lower())\n",
    "    return tokens_lower\n",
    "\n",
    "def add_padding(tokens:list):\n",
    "    if isinstance(tokens[0], str):\n",
    "        return tokens, len(tokens)\n",
    "    elif isinstance(tokens[0], list):\n",
    "        tokens = copy.deepcopy(tokens)\n",
    "        max_len = 0\n",
    "        for seq in tokens:\n",
    "            if len(seq) > max_len:\n",
    "                max_len = len(seq)\n",
    "        for seq in tokens:\n",
    "            i = len(seq)\n",
    "            while i < max_len:\n",
    "                seq.append('')\n",
    "                i += 1\n",
    "        return tokens\n",
    "    else:\n",
    "        raise Exception('tokens should be either list of strings or list of lists of strings')\n",
    "  \n",
    "def getNeTagMainPart(tag:str):\n",
    "    return tag[2:] if len(tag) > 2 else tag\n",
    "\n",
    "def tags2binaryFlat(tags):\n",
    "    return np.array([1 if t == 'T' or (len(t) > 2 and t[2:] == 'T') else 0 for seq in tags for t in seq])\n",
    "\n",
    "def tagsEncodePadded(tags:list, binary=True, tag2idx=None):\n",
    "    if tag2idx:\n",
    "        binary = False\n",
    "    if isinstance(tags[0], str):\n",
    "        tags = [tags]\n",
    "    n_sentences = len(tags)\n",
    "    tokens_length = get_tokens_len(tags)\n",
    "    max_len = np.max(tokens_length)\n",
    "    y = np.zeros((n_sentences, max_len))\n",
    "    for i, sen in enumerate(tags):\n",
    "        for j, tag in enumerate(sen):\n",
    "            if binary: \n",
    "                y[i][j] = 1 if tags[i][j] != 'O' else 0\n",
    "            else:\n",
    "                if tag2idx:\n",
    "#                     tag_name = tag if USE_BIO_MARKUP else getNeTagMainPart(tag)\n",
    "                    tag_name = tag   # in case when BIO markup was deleted from data\n",
    "                    y[i][j] = tag2idx[tag_name]\n",
    "                else:\n",
    "                    raise Exception('tag2idx dictionary should be provided')\n",
    "    return y\n",
    "\n",
    "def get_matrices(tokens, tags, embedder):\n",
    "    return (embeddings2feat_mat(embedder.embed(tokens), get_tokens_len(tokens)),\n",
    "           tags2binaryFlat(tags))\n",
    "  \n",
    "def split_tokens_tags(dataset: list):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for sample in dataset:\n",
    "        tokens.append(sample[0])\n",
    "        tags.append(sample[1])\n",
    "    return tokens, tags\n",
    "\n",
    "def count_tags(dataset: list):\n",
    "    tag_counts = {}\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        tag_counts[data_type] = {}\n",
    "        for sen in dataset[data_type]:\n",
    "            tags = sen[1]\n",
    "            for tag in tags:\n",
    "                if tag_counts[data_type].get(tag):\n",
    "                    tag_counts[data_type][tag] += 1\n",
    "                else:\n",
    "                    tag_counts[data_type][tag] = 1\n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11467,
     "status": "ok",
     "timestamp": 1535754891200,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "gKtBm_jBPaAt",
    "outputId": "b9bf0ed3-d79a-45e2-be3c-3de8fec4c838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train sentences: 75187\n",
      "Num of valid sentences: 9603\n",
      "Num of test sentences: 9479\n",
      "[(['Actions', 'had', 'to', 'be', 'taken', 'to', 'break', 'through', 'the', 'blockade', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'a', 'night', 'in', 'late', 'July', '1940', ',', 'the', 'atmosphere', 'in', 'Zhuanbi', 'Village', 'in', 'Shaanxi', 'was', 'unusual', '.'], ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'B-GPE', 'O', 'O', 'O']), (['Villager', 'Xiao', 'Jianghe', 'has', 'a', 'vivid', 'memory', 'of', 'this', 'piece', 'of', 'history', '.'], ['O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['On', 'that', 'dark', 'night', ',', 'everyone', 'was', 'sleeping', 'when', 'human', 'voices', 'and', 'neighing', 'horses', 'were', 'heard', 'within', 'the', 'village', '.'], ['O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['People', 'all', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Did', 'something', 'happen', '?'], ['O', 'O', 'O', 'O']), (['Some', 'folks', 'got', 'up', '.'], ['O', 'O', 'O', 'O', 'O']), (['Opening', 'the', 'street', 'gate', ',', 'they', 'saw', 'a', 'soldier', 'standing', 'by', 'the', 'gate', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Folks', ',', 'go', 'back', ',', 'go', 'back', ',', 'nothing', 'is', 'wrong', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Our', 'troops', 'are', 'just', 'going', 'to', 'stay', 'here', 'for', 'the', 'night', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n",
      "NE types used for training:\n",
      "['GPE', 'DATE', 'ORG', 'EVENT', 'LOC', 'FAC', 'CARDINAL', 'QUANTITY', 'NORP', 'ORDINAL', 'WORK_OF_ART', 'LANGUAGE', 'TIME', 'PRODUCT', 'MONEY', 'LAW', 'PERCENT']\n",
      "17 in total\n",
      "Num of train sentences: 28872\n",
      "Num of valid sentences: 3975\n",
      "Num of test sentences: 4026\n",
      "Number of sentences in filtered dataset: train: 28872, valid: 3975, test: 4026\n",
      "Tag counts:\n",
      "{'train': {'O': 597244, 'B-ORG': 12820, 'I-ORG': 18246, 'B-WORK_OF_ART': 974, 'I-WORK_OF_ART': 2400, 'B-LOC': 1514, 'I-LOC': 1395, 'B-CARDINAL': 7367, 'B-EVENT': 748, 'I-EVENT': 1605, 'B-NORP': 6870, 'B-GPE': 15405, 'I-GPE': 3679, 'B-DATE': 10922, 'I-DATE': 13333, 'B-FAC': 860, 'I-FAC': 1467, 'B-QUANTITY': 657, 'I-QUANTITY': 1235, 'B-ORDINAL': 1640, 'B-TIME': 1233, 'I-TIME': 1507, 'I-CARDINAL': 2289, 'B-PRODUCT': 606, 'I-PRODUCT': 576, 'I-NORP': 446, 'B-PERCENT': 1763, 'I-PERCENT': 2498, 'B-MONEY': 2434, 'I-MONEY': 4920, 'B-LAW': 282, 'I-LAW': 785, 'B-LANGUAGE': 304, 'I-LANGUAGE': 13, 'I-ORDINAL': 5}, 'valid': {'O': 80615, 'B-DATE': 1507, 'I-DATE': 1809, 'B-GPE': 2268, 'I-GPE': 555, 'B-ORDINAL': 232, 'B-ORG': 1740, 'B-FAC': 115, 'I-FAC': 203, 'I-ORG': 2336, 'B-QUANTITY': 100, 'I-QUANTITY': 209, 'B-LOC': 204, 'I-LOC': 188, 'B-CARDINAL': 938, 'B-NORP': 847, 'B-LAW': 40, 'I-LAW': 84, 'I-CARDINAL': 290, 'B-MONEY': 274, 'I-MONEY': 587, 'B-TIME': 214, 'B-EVENT': 143, 'I-EVENT': 272, 'B-WORK_OF_ART': 142, 'I-WORK_OF_ART': 334, 'I-TIME': 260, 'B-PERCENT': 177, 'I-PERCENT': 258, 'I-NORP': 44, 'B-PRODUCT': 72, 'I-PRODUCT': 129, 'B-LANGUAGE': 33, 'I-ORDINAL': 4}, 'test': {'O': 82515, 'B-NORP': 841, 'B-ORG': 1795, 'I-ORG': 2406, 'B-DATE': 1602, 'B-GPE': 2240, 'I-DATE': 2011, 'B-FAC': 135, 'I-FAC': 213, 'I-GPE': 628, 'B-CARDINAL': 935, 'B-TIME': 212, 'I-TIME': 255, 'B-ORDINAL': 195, 'B-EVENT': 63, 'I-EVENT': 130, 'I-CARDINAL': 331, 'B-QUANTITY': 105, 'I-QUANTITY': 206, 'B-PERCENT': 349, 'I-PERCENT': 523, 'B-LOC': 179, 'I-LOC': 180, 'B-WORK_OF_ART': 166, 'I-WORK_OF_ART': 337, 'B-MONEY': 314, 'I-MONEY': 685, 'B-LAW': 40, 'I-LAW': 106, 'I-NORP': 160, 'I-ORDINAL': 4, 'B-PRODUCT': 76, 'I-PRODUCT': 69, 'B-LANGUAGE': 22}}\n"
     ]
    }
   ],
   "source": [
    "dataset_orig = read_data()\n",
    "ne_types_holdout = ['PERSON']\n",
    "ne_types_all = ['GPE','DATE','ORG','EVENT','LOC','FAC','CARDINAL','QUANTITY','NORP','ORDINAL','WORK_OF_ART', 'LANGUAGE', 'TIME', 'PRODUCT', 'MONEY', 'LAW', 'PERCENT', 'PERSON']\n",
    "ne_types = [t for t in ne_types_all if t not in ne_types_holdout]\n",
    "# ne_types = ['PERSON', 'ORG']\n",
    "# ne_types = ['LOC', 'EVENT']\n",
    "print('NE types used for training:')\n",
    "print(ne_types)\n",
    "print('{} in total'.format(len(ne_types)))\n",
    "dataset = filter_dataset_by_ne_types(dataset_orig, ne_types, tags2binary=False, preserveBIO=USE_BIO_MARKUP, keepIfAny=True)\n",
    "print('Number of sentences in filtered dataset: train: {}, valid: {}, test: {}'.format(len(dataset['train']), len(dataset['valid']), len(dataset['test'])))\n",
    "tag_counts = count_tags(dataset)\n",
    "print('Tag counts:')\n",
    "print(tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1535754891874,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "T74bOXjxPaAw",
    "outputId": "8b445d09-09ec-40db-aa2d-193270babf83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'respectfully', 'invite', 'you', 'to', 'watch', 'a', 'special', 'edition', 'of', 'Across', 'China', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O']), (['WW', 'II', 'Landmarks', 'on', 'the', 'Great', 'Earth', 'of', 'China', ':', 'Eternal', 'Memories', 'of', 'Taihang', 'Mountain'], ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']), (['Standing', 'tall', 'on', 'Taihang', 'Mountain', 'is', 'the', 'Monument', 'to', 'the', 'Hundred', 'Regiments', 'Offensive', '.'], ['O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O']), (['It', 'is', 'composed', 'of', 'a', 'primary', 'stele', ',', 'secondary', 'steles', ',', 'a', 'huge', 'round', 'sculpture', 'and', 'beacon', 'tower', ',', 'and', 'the', 'Great', 'Wall', ',', 'among', 'other', 'things', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O']), (['A', 'primary', 'stele', ',', 'three', 'secondary', 'steles', ',', 'and', 'two', 'inscribed', 'steles', '.'], ['O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O']), (['The', 'Hundred', 'Regiments', 'Offensive', 'was', 'the', 'campaign', 'of', 'the', 'largest', 'scale', 'launched', 'by', 'the', 'Eighth', 'Route', 'Army', 'during', 'the', 'War', 'of', 'Resistance', 'against', 'Japan', '.'], ['B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O']), (['This', 'campaign', 'broke', 'through', 'the', 'Japanese', 'army', \"'s\", 'blockade', 'to', 'reach', 'base', 'areas', 'behind', 'enemy', 'lines', ',', 'stirring', 'up', 'anti-Japanese', 'spirit', 'throughout', 'the', 'nation', 'and', 'influencing', 'the', 'situation', 'of', 'the', 'anti-fascist', 'war', 'of', 'the', 'people', 'worldwide', '.'], ['O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['This', 'is', 'Zhuanbi', 'Village', ',', 'Wuxiang', 'County', 'of', 'Shanxi', 'Province', ',', 'where', 'the', 'Eighth', 'Route', 'Army', 'was', 'headquartered', 'back', 'then', '.'], ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'I-GPE', 'I-GPE', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O']), (['This', 'map', 'was', 'the', 'Eighth', 'Route', 'Army', \"'s\", 'depiction', 'of', 'the', 'Mediterranean', 'Sea', 'situation', 'at', 'that', 'time', '.'], ['O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O']), (['This', 'map', 'reflected', 'the', 'European', 'battlefield', 'situation', '.'], ['O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1535754892805,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "KkFrEV5kPaAz",
    "outputId": "103de6d6-d2ac-4f52-c3f9-fc5d76d8c336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-GPE': 1, 'I-GPE': 2, 'B-DATE': 3, 'I-DATE': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-EVENT': 7, 'I-EVENT': 8, 'B-LOC': 9, 'I-LOC': 10, 'B-FAC': 11, 'I-FAC': 12, 'B-CARDINAL': 13, 'I-CARDINAL': 14, 'B-QUANTITY': 15, 'I-QUANTITY': 16, 'B-NORP': 17, 'I-NORP': 18, 'B-ORDINAL': 19, 'I-ORDINAL': 20, 'B-WORK_OF_ART': 21, 'I-WORK_OF_ART': 22, 'B-LANGUAGE': 23, 'I-LANGUAGE': 24, 'B-TIME': 25, 'I-TIME': 26, 'B-PRODUCT': 27, 'I-PRODUCT': 28, 'B-MONEY': 29, 'I-MONEY': 30, 'B-LAW': 31, 'I-LAW': 32, 'B-PERCENT': 33, 'I-PERCENT': 34}\n",
      "['O', 'B-GPE', 'I-GPE', 'B-DATE', 'I-DATE', 'B-ORG', 'I-ORG', 'B-EVENT', 'I-EVENT', 'B-LOC', 'I-LOC', 'B-FAC', 'I-FAC', 'B-CARDINAL', 'I-CARDINAL', 'B-QUANTITY', 'I-QUANTITY', 'B-NORP', 'I-NORP', 'B-ORDINAL', 'I-ORDINAL', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LANGUAGE', 'I-LANGUAGE', 'B-TIME', 'I-TIME', 'B-PRODUCT', 'I-PRODUCT', 'B-MONEY', 'I-MONEY', 'B-LAW', 'I-LAW', 'B-PERCENT', 'I-PERCENT']\n"
     ]
    }
   ],
   "source": [
    "tag2idx = {}\n",
    "idx2tag = []\n",
    "tag2idx['O'] = 0\n",
    "idx2tag.append('O')\n",
    "i = 1\n",
    "for tag in ne_types:\n",
    "    if USE_BIO_MARKUP:\n",
    "        tag2idx['B-'+tag] = i\n",
    "        tag2idx['I-'+tag] = i+1\n",
    "        idx2tag.append('B-'+tag)\n",
    "        idx2tag.append('I-'+tag)\n",
    "        i += 2\n",
    "    else:\n",
    "        tag2idx[tag] = i\n",
    "        idx2tag.append(tag)\n",
    "        i += 1\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZK3jHAGzPaA2"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPU_AVAILABLE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-43f176f69ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_gpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPU_AVAILABLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mINITIALIZER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m def cudnn_lstm_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n\u001b[1;32m      5\u001b[0m                        initial_c=None, name='cudnn_lstm', reuse=False):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GPU_AVAILABLE'"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.common.check_gpu import GPU_AVAILABLE\n",
    "INITIALIZER = tf.contrib.layers.xavier_initializer\n",
    "\n",
    "def cudnn_lstm_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "                       initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "\n",
    "    if GPU_AVAILABLE:\n",
    "        return cudnn_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                          seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "    log.info('\\nWarning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. '\n",
    "             'It is okay for inference mode, but '\n",
    "             'if you train your model with this cell it could NOT be used with '\n",
    "             'tf.contrib.cudnn_rnn.CudnnLSTMCell later. '\n",
    "             )\n",
    "\n",
    "    return cudnn_compatible_lstm(units, n_hidden, n_layers, trainable_initial_states,\n",
    "                                 seq_lengths, initial_h, initial_c, name, reuse)\n",
    "\n",
    "def cudnn_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n",
    "               initial_c=None, name='cudnn_lstm', reuse=False):\n",
    "    \"\"\" Fast CuDNN LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            seq_lengths: tensor of sequence lengths with dimension [B]\n",
    "            initial_h: optional initial hidden state, masks trainable_initial_states\n",
    "                if provided\n",
    "            initial_c: optional initial cell state, masks trainable_initial_states\n",
    "                if provided\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x (n_layers * H)]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=n_layers,\n",
    "                                              num_units=n_hidden)\n",
    "        if trainable_initial_states:\n",
    "            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n",
    "            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n",
    "            init_c = tf.get_variable('init_c', [n_layers, 1, n_hidden])\n",
    "            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n",
    "        else:\n",
    "            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n",
    "\n",
    "        initial_h = initial_h or init_h\n",
    "        initial_c = initial_c or init_c\n",
    "\n",
    "        h, (h_last, c_last) = lstm(tf.transpose(units, (1, 0, 2)), (initial_h, initial_c))\n",
    "        h = tf.transpose(h, (1, 0, 2))\n",
    "        h_last = tf.reshape(h_last, shape=(-1, n_hidden))\n",
    "        c_last = tf.reshape(c_last, shape=(-1, n_hidden))\n",
    "\n",
    "        # Extract last states if they are provided\n",
    "        if seq_lengths is not None:\n",
    "            indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1)\n",
    "            h_last = tf.gather_nd(h, indices)\n",
    "\n",
    "        return h, (h_last, c_last)\n",
    "\n",
    "def cudnn_bi_lstm(units,\n",
    "                  n_hidden,\n",
    "                  seq_lengths=None,\n",
    "                  n_layers=1,\n",
    "                  trainable_initial_states=False,\n",
    "                  name='cudnn_bi_lstm',\n",
    "                  reuse=False):\n",
    "    \"\"\" Fast CuDNN Bi-LSTM implementation\n",
    "\n",
    "        Args:\n",
    "            units: tf.Tensor with dimensions [B x T x F], where\n",
    "                B - batch size\n",
    "                T - number of tokens\n",
    "                F - features\n",
    "            n_hidden: dimensionality of hidden state\n",
    "            seq_lengths: number of tokens in each sample in the batch\n",
    "            n_layers: number of layers\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            name: name of the variable scope to use\n",
    "            reuse:whether to reuse already initialized variable\n",
    "\n",
    "        Returns:\n",
    "            h - all hidden states along T dimension,\n",
    "                tf.Tensor with dimensionality [B x T x F]\n",
    "            h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "            c_last - last cell state, tf.Tensor with dimensionality [B x H * 2]\n",
    "                where H - number of hidden units\n",
    "        \"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        if seq_lengths is None:\n",
    "            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n",
    "        with tf.variable_scope('Forward'):\n",
    "            h_fw, (h_fw_last, c_fw_last) = cudnn_lstm_wrapper(units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "        with tf.variable_scope('Backward'):\n",
    "            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "            h_bw, (h_bw_last, c_bw_last) = cudnn_lstm_wrapper(reversed_units,\n",
    "                                                              n_hidden,\n",
    "                                                              n_layers=n_layers,\n",
    "                                                              trainable_initial_states=trainable_initial_states,\n",
    "                                                              seq_lengths=seq_lengths)\n",
    "\n",
    "            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n",
    "        return (h_fw, h_bw), ((h_fw_last, c_fw_last), (h_bw_last, c_bw_last))\n",
    "\n",
    "def bi_rnn(units: tf.Tensor,\n",
    "           n_hidden: List,\n",
    "           cell_type='gru',\n",
    "           seq_lengths=None,\n",
    "           trainable_initial_states=False,\n",
    "           use_peepholes=False,\n",
    "           name='Bi-'):\n",
    "    \"\"\" Bi directional recurrent neural network. GRU or LSTM\n",
    "\n",
    "        Args:\n",
    "            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n",
    "            n_hidden_list: list with number of hidden units at the ouput of each layer\n",
    "            seq_lengths: length of sequences for different length sequences in batch\n",
    "                can be None for maximum length as a length for every sample in the batch\n",
    "            cell_type: 'lstm' or 'gru'\n",
    "            trainable_initial_states: whether to create a special trainable variable\n",
    "                to initialize the hidden states of the network or use just zeros\n",
    "            use_peepholes: whether to use peephole connections (only 'lstm' case affected)\n",
    "            name: what variable_scope to use for the network parameters\n",
    "            add_l2_losses: whether to add l2 losses on network kernels to\n",
    "                tf.GraphKeys.REGULARIZATION_LOSSES or not\n",
    "        Returns:\n",
    "            units: tensor at the output of the last recurrent layer\n",
    "                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n",
    "            last_units: tensor of last hidden states for GRU and tuple\n",
    "                of last hidden stated and last cell states for LSTM\n",
    "                dimensionality of cell states and hidden states are\n",
    "                similar and equal to [B x 2 * H], where B - batch\n",
    "                size and H is number of hidden units\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(name + '_' + cell_type.upper()):\n",
    "        if cell_type == 'gru':\n",
    "            forward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "                initial_state_bw = tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        elif cell_type == 'lstm':\n",
    "            forward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            backward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n",
    "            if trainable_initial_states:\n",
    "                initial_state_fw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_fw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_fw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "                initial_state_bw = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    tf.tile(tf.get_variable('init_bw_c', [1, n_hidden]), (tf.shape(units)[0], 1)),\n",
    "                    tf.tile(tf.get_variable('init_bw_h', [1, n_hidden]), (tf.shape(units)[0], 1)))\n",
    "            else:\n",
    "                initial_state_fw = initial_state_bw = None\n",
    "        else:\n",
    "            raise RuntimeError('cell_type must be either \"gru\" or \"lstm\"s')\n",
    "        (rnn_output_fw, rnn_output_bw), (fw, bw) = \\\n",
    "            tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                            backward_cell,\n",
    "                                            units,\n",
    "                                            dtype=tf.float32,\n",
    "                                            sequence_length=seq_lengths,\n",
    "                                            initial_state_fw=initial_state_fw,\n",
    "                                            initial_state_bw=initial_state_bw)\n",
    "    kernels = [var for var in forward_cell.trainable_variables +\n",
    "               backward_cell.trainable_variables if 'kernel' in var.name]\n",
    "    for kernel in kernels:\n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(kernel))\n",
    "    return (rnn_output_fw, rnn_output_bw), (fw, bw)\n",
    "\n",
    "def build_cudnn_rnn(units, mask, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        with tf.variable_scope(cell_type.upper() + '_' + str(n)):\n",
    "            if cell_type.lower() == 'lstm':\n",
    "                units, _ = cudnn_bi_lstm(units, n_hidden, sequence_lengths)\n",
    "            elif cell_type.lower() == 'gru':\n",
    "                units, _ = cudnn_bi_gru(units, n_hidden, sequence_lengths)\n",
    "            else:\n",
    "                raise RuntimeError('Wrong cell type \"{}\"! Only \"gru\" and \"lstm\"!'.format(cell_type))\n",
    "            units = tf.concat(units, -1)\n",
    "            if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "                units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_rnn(units, n_hidden_list:Tuple[int]=(128,), cell_type:str='lstm', intra_layer_dropout:bool=False, dropout_ph=None):\n",
    "    for n, n_hidden in enumerate(n_hidden_list):\n",
    "        units, _ = bi_rnn(units, n_hidden, cell_type=cell_type, name='Layer_' + str(n))\n",
    "        units = tf.concat(units, -1)\n",
    "        if intra_layer_dropout and n != len(n_hidden_list) - 1:\n",
    "            units = variational_dropout(units, dropout_ph)\n",
    "    return units\n",
    "\n",
    "def build_top(units, n_tags=18*2+1, top_dropout:bool=False, two_dense_on_top:bool=False, n_hidden=128):\n",
    "    if top_dropout:\n",
    "        units = variational_dropout(units, dropout_ph)\n",
    "    if two_dense_on_top:\n",
    "        units = tf.layers.dense(units, n_hidden, activation=tf.nn.relu,\n",
    "                                kernel_initializer=INITIALIZER(),\n",
    "                                kernel_regularizer=tf.nn.l2_loss)\n",
    "    logits = tf.layers.dense(units, n_tags, activation=None,\n",
    "                             kernel_initializer=INITIALIZER(),\n",
    "                             kernel_regularizer=tf.nn.l2_loss)\n",
    "    return logits\n",
    "\n",
    "def build_train_predict(logits, n_tags, mask, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg):\n",
    "    res = {}\n",
    "    if use_crf:\n",
    "        sequence_lengths = tf.reduce_sum(mask, axis=1)\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(logits, y_ph, sequence_lengths)\n",
    "        loss_tensor = -log_likelihood\n",
    "        res['transition_params'] = transition_params\n",
    "    else:\n",
    "        ground_truth_labels = tf.one_hot(y_ph, n_tags)\n",
    "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_labels, logits=logits)\n",
    "        loss_tensor = loss_tensor * mask\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        res['y_pred'] = y_pred\n",
    "\n",
    "    loss = tf.reduce_mean(loss_tensor)\n",
    "\n",
    "    # L2 regularization\n",
    "    if l2_reg > 0:\n",
    "        loss += l2_reg * tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    res['loss'] = loss\n",
    "        \n",
    "    # optimizer = partial(tf.train.MomentumOptimizer, momentum=0.9, use_nesterov=True)\n",
    "    optimizer = tf.train.AdamOptimizer\n",
    "    train_op = get_train_op(loss, learning_rate_ph, optimizer, clip_norm=clip_grad_norm)\n",
    "    res['train_op'] = train_op\n",
    "    return res\n",
    "\n",
    "def predict_no_crf(y_pred, mask_ph, feed_dict):\n",
    "    pred_idxs, mask = sess.run([y_pred, mask_ph], feed_dict)\n",
    "\n",
    "    # Filter by sequece length\n",
    "    sequence_lengths = np.sum(mask, axis=1).astype(np.int32)\n",
    "    pred = []\n",
    "    for utt, l in zip(pred_idxs, sequence_lengths):\n",
    "        pred.append(utt[:l])\n",
    "    return pred\n",
    "\n",
    "def predict_crf(logits, transition_params, mask_ph, feed_dict):\n",
    "    logits, trans_params, mask = sess.run([logits,\n",
    "                                           transition_params,\n",
    "                                           mask_ph],\n",
    "                                           feed_dict=feed_dict)\n",
    "    sequence_lengths = np.maximum(np.sum(mask, axis=1).astype(np.int32), 1)\n",
    "    # iterate over the sentences because no batching in viterbi_decode\n",
    "    pred = []\n",
    "    for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "        logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
    "        viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "        pred += [viterbi_seq]\n",
    "    return pred\n",
    "\n",
    "def get_train_op(loss,\n",
    "                 learning_rate,\n",
    "                 optimizer=None,\n",
    "                 clip_norm=None,\n",
    "                 learnable_scopes=None,\n",
    "                 optimizer_scope_name=None,\n",
    "                 trainable_vars:list=None):\n",
    "    \"\"\" Get train operation for given loss\n",
    "\n",
    "    Args:\n",
    "        loss: loss, tf tensor or scalar\n",
    "        learning_rate: scalar or placeholder\n",
    "        clip_norm: clip gradients norm by clip_norm\n",
    "        learnable_scopes: which scopes are trainable (None for all)\n",
    "        optimizer: instance of tf.train.Optimizer, default Adam\n",
    "\n",
    "    Returns:\n",
    "        train_op\n",
    "    \"\"\"\n",
    "    if optimizer_scope_name is None:\n",
    "        opt_scope = tf.variable_scope('Optimizer')\n",
    "    else:\n",
    "        opt_scope = tf.variable_scope(optimizer_scope_name)\n",
    "    with opt_scope:\n",
    "        if learnable_scopes is None:\n",
    "            variables_to_train = tf.global_variables()\n",
    "        else:\n",
    "            variables_to_train = []\n",
    "            for scope_name in learnable_scopes:\n",
    "                for var in tf.global_variables():\n",
    "                    if scope_name in var.name:\n",
    "                        variables_to_train.append(var)\n",
    "        if trainable_vars:\n",
    "            variables_to_train = trainable_vars\n",
    "            \n",
    "        if optimizer is None:\n",
    "            optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "        # For batch norm it is necessary to update running averages\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            opt = optimizer(learning_rate)\n",
    "            grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n",
    "            if clip_norm is not None:\n",
    "                grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                                  for grad, var in grads_and_vars] #  if grad is not None\n",
    "            train_op = opt.apply_gradients(grads_and_vars)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cfEVVG1PaA4"
   },
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size=None, binaryTags=True, tag2idx=None):\n",
    "    if not batch_size:\n",
    "        batch_size = len(dataset)\n",
    "    tokens, tags = get_data_sample(dataset, batch_size)\n",
    "    mask = make_mask(tokens)\n",
    "    tokens_length = get_tokens_len(tokens)\n",
    "    tokens = add_padding(tokens)\n",
    "    y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "    return tokens, tags, mask, y\n",
    "\n",
    "def make_mask(seq_list):\n",
    "  seq_count = len(seq_list)\n",
    "  seq_length = [len(s) for s in seq_list]\n",
    "  max_len = np.max(seq_length)\n",
    "  mask = np.zeros((seq_count, max_len), dtype=int)\n",
    "  seq_length = np.tile(np.expand_dims(seq_length, axis=-1), (1, max_len))\n",
    "  range_ar = np.tile(np.arange(1, max_len+1, 1), (seq_count, 1))\n",
    "  mask[range_ar <= seq_length] = 1\n",
    "  return mask\n",
    "\n",
    "def flatten_with_mask(seq_mat, mask):\n",
    "  return seq_mat[mask == 1]\n",
    "\n",
    "def concatenate_arrays(ar_list):\n",
    "  return np.concatenate(ar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHZPI7b3PaA7"
   },
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, data):\n",
    "        self.data = {\n",
    "            'train': data['train'],\n",
    "            'valid': data['valid'],\n",
    "            'test': data['test']\n",
    "        }\n",
    "        \n",
    "    def get_samples_count(self, data_type='train'):\n",
    "        return len(self.data[data_type])\n",
    "\n",
    "    def gen_batches(self, batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=None):\n",
    "        indices = np.arange(len(self.data[data_type]))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        n = indices.size\n",
    "        k = 0\n",
    "        while k < n:\n",
    "            top = k + batch_size\n",
    "            if top > n: \n",
    "                top = n\n",
    "            indices_batch = indices[k:top]\n",
    "            tokens,tags = split_tokens_tags([self.data[data_type][i] for i in indices_batch])\n",
    "            mask = make_mask(tokens)\n",
    "            tokens_length = get_tokens_len(tokens)\n",
    "            tokens = add_padding(tokens)\n",
    "            y = tagsEncodePadded(tags, binary=binaryTags, tag2idx=tag2idx)\n",
    "            yield tokens, tags, mask, y\n",
    "            k += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFG9BPwlPaA9"
   },
   "outputs": [],
   "source": [
    "def eval_valid(batch_generator, tag2idx):\n",
    "    pred_total = []\n",
    "    true_total = []\n",
    "    loss_valid_total = 0\n",
    "    for i, (tokens_valid, tags_valid, mask_valid, y_valid) in enumerate(batch_generator.gen_batches(32, data_type='valid', shuffle=False, binaryTags=False, tag2idx=tag2idx)):\n",
    "        feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "        y_valid_flat = flatten_with_mask(y_valid, mask_valid)\n",
    "        loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "        if use_crf:\n",
    "            pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "        else:\n",
    "            pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "#         print('y_true: {}'.format(y_valid))\n",
    "#         print('y_pred: {}'.format(pred))\n",
    "        pred = concatenate_arrays(pred)\n",
    "        loss_valid_total += loss_valid\n",
    "        pred_total = np.concatenate([pred_total, pred])\n",
    "        true_total = np.concatenate([true_total, y_valid_flat])\n",
    "#     print(np.unique(true_total))\n",
    "#     print(np.unique(pred_total))\n",
    "#     print(true_total.size == pred_total.size)\n",
    "#     a = np.arange(true_total.size)\n",
    "#     print(a[true_total == 2])\n",
    "#     print(a[pred_total == 2])\n",
    "    n_tags = len(tag2idx.keys())\n",
    "    f1_valid = f1_score(true_total, pred_total, average=None, labels=list(range(1,n_tags)))\n",
    "#     print(f1_valid)\n",
    "    loss_valid_total = loss_valid_total/(i+1)\n",
    "    \n",
    "    return {'loss': loss_valid_total, 'f1': f1_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2WmzRJuPaBB"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiXGWeaSPaBD"
   },
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=TRAIN_ELMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1535754900840,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "fIemR3FYPaBI",
    "outputId": "d1e86786-96ed-42c0-b74d-9d5c434f53be"
   },
   "outputs": [],
   "source": [
    "print(tf.trainable_variables())\n",
    "if(TRAIN_ELMO):\n",
    "    elmo_coef = {'layer_coefficients': tf.trainable_variables()[-2], 'scaling': tf.trainable_variables()[-1]}\n",
    "    print(elmo_coef)\n",
    "elmo_vars = tf.trainable_variables()\n",
    "elmo_vars_coef = list(elmo_coef.values())\n",
    "elmo_vars_cell_weights = [v for v in elmo_vars if v not in elmo_vars_coef]\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}\n",
    "if TRAIN_ALL_ELMO_PARAMS:\n",
    "    cell0_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0']\n",
    "    cell1_kernel = vars_dict['module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_1Gj9SCPaBK"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "use_cudnn_rnn = True\n",
    "l2_reg = 0\n",
    "n_hidden_list = (128,)\n",
    "cell_type = 'lstm'\n",
    "n_tags = len(idx2tag)\n",
    "use_crf = True\n",
    "clip_grad_norm = 5.0\n",
    "learning_rate = 1e-3\n",
    "dropout_keep_prob = 0.5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiD7H4jnPaBN"
   },
   "source": [
    "### Build computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9Ja_jdYPaBN"
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "tokens_input_ph = tf.placeholder(shape=[None, None], dtype=tf.string)\n",
    "# tokens_length_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "mask_ph = tf.placeholder(tf.float32, [None, None], name='Mask_ph')\n",
    "y_ph = tf.placeholder(shape=[None, None], dtype=tf.int32, name='y_ph')\n",
    "learning_rate_ph = tf.placeholder_with_default(learning_rate, shape=[], name='learning_rate')\n",
    "dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name='dropout')\n",
    "training_ph = tf.placeholder_with_default(False, shape=[], name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PElhM3v9PaBQ"
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(inp: dict, train=True):\n",
    "    feed_dict = {learning_rate_ph: learning_rate, dropout_ph: dropout_keep_prob if train else 1.0, training_ph: train}\n",
    "    feed_dict.update(inp)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7763,
     "status": "ok",
     "timestamp": 1535754911225,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "2k__QsnjPaBU",
    "outputId": "c5837ecf-def7-4a1e-fb33-eae4a87e691d"
   },
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "transition_params = None\n",
    "tokens_length = tf.to_int32(tf.reduce_sum(mask_ph, axis=1))\n",
    "emb = elmo(inputs={\"tokens\": tokens_input_ph,\n",
    "                    \"sequence_len\": tokens_length},\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)['elmo']\n",
    "# mask = tf.sequence_mask(lengths=tokens_length_ph, dtype=tf.float32)\n",
    "features = emb\n",
    "if use_cudnn_rnn:\n",
    "    units = build_cudnn_rnn(features, mask_ph, n_hidden_list, cell_type)\n",
    "else:\n",
    "    units = build_rnn(features, n_hidden_list, cell_type)\n",
    "\n",
    "logits = build_top(units, n_tags=n_tags)\n",
    "\n",
    "out_dict = build_train_predict(logits, n_tags, mask_ph, y_ph, use_crf, learning_rate_ph, clip_grad_norm, l2_reg)\n",
    "train_op_all = out_dict['train_op']\n",
    "loss = out_dict['loss']\n",
    "if use_crf:\n",
    "    transition_params = out_dict['transition_params']\n",
    "else:\n",
    "    y_pred = out_dict['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1535754911923,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "Girz7LPfPaBZ",
    "outputId": "108a2794-fd9a-43e6-e6e9-9d75d3b5eb46"
   },
   "outputs": [],
   "source": [
    "all_vars = tf.trainable_variables()\n",
    "model_vars = [v for v in all_vars if v not in elmo_vars]\n",
    "print(model_vars)\n",
    "vars_dict = {v.name:v for v in tf.trainable_variables()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8061,
     "status": "ok",
     "timestamp": 1535754920250,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "nYhuNEucPaBe",
    "outputId": "81407e25-7cb5-4e47-bc96-8174ada4ee7b"
   },
   "outputs": [],
   "source": [
    "# Optimizers for different parameters\n",
    "with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE):\n",
    "    train_op_model = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=model_vars)\n",
    "    train_op_elmo = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars)\n",
    "    train_op_elmo_coef = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_coef)\n",
    "    train_op_elmo_cell_weights = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_cell_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zC9DxF3EPaBk"
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('./graph/bilstm_crf_elmo_bio_multi', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlqOMRZ_kVuf"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6190,
     "status": "ok",
     "timestamp": 1535754931375,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "7m5dO-N2PaBp",
    "outputId": "6ed6a02d-2b7b-4fd8-9d8e-8a1ce5f21aae"
   },
   "outputs": [],
   "source": [
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run([initialize_op])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqnoc3KbPaBt"
   },
   "outputs": [],
   "source": [
    "dataset_iterator = DatasetIterator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNPK6IDSPaBv"
   },
   "outputs": [],
   "source": [
    "valid_sen_size = 100\n",
    "tokens_valid, tags_valid, mask_valid, y_valid = get_batch(dataset['valid'], valid_sen_size, binaryTags=False, tag2idx=tag2idx)\n",
    "feed_valid = fill_feed_dict({tokens_input_ph: tokens_valid, mask_ph: mask_valid, y_ph: y_valid, training_ph: False}, train=False)\n",
    "y_valid_flat = flatten_with_mask(y_valid, mask_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs3vYbzrPaBy"
   },
   "outputs": [],
   "source": [
    "# print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3KZRiO9PaB3"
   },
   "outputs": [],
   "source": [
    "# print(tags_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1535754935298,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "PyIP0uJyPaB5",
    "outputId": "ac8f38b8-f010-42f3-86b5-d62ec1083968"
   },
   "outputs": [],
   "source": [
    "print(np.unique(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dOdH8vdPaB8"
   },
   "outputs": [],
   "source": [
    "# training_schedule = [{'train_op': train_op_model, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_cell_weights, 'n_epochs': 200, 'lr': 1e-3}, {'train_op': train_op_elmo_coef, 'n_epochs': 200, 'lr': 1e-2}]\n",
    "training_schedule = [{'train_op': train_op_model, 'n_epochs': 10, 'lr': 3e-3}, {'train_op': train_op_elmo_cell_weights, 'n_epochs': 10, 'lr': 1e-3}, {'train_op': train_op_elmo_coef, 'n_epochs': 10, 'lr': 1e-2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "APd8Zl-9PaB-",
    "outputId": "5141d44f-1b23-4529-f6b5-8edc727f2c4f"
   },
   "outputs": [],
   "source": [
    "num_epochs = np.sum([s['n_epochs'] for s in training_schedule])\n",
    "stage = 0\n",
    "n_epochs_prev_stages = 0\n",
    "display_epoch = 1\n",
    "valid_epoch = 1\n",
    "losses_train = []\n",
    "losses_epoch = {'train': [], 'valid': []}\n",
    "f1_scores = {'train': [], 'valid': []}\n",
    "best_valid_f1 = 0\n",
    "d_elmo_cells_list = {'cell0':[], 'cell1':[]}\n",
    "n_batches_train = ceil(len(dataset['train'])/batch_size)\n",
    "step = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "    if epoch > n_epochs_prev_stages + training_schedule[stage]['n_epochs']:\n",
    "        n_epochs_prev_stages += training_schedule[stage]['n_epochs']\n",
    "        stage += 1\n",
    "    train_op = training_schedule[stage]['train_op']\n",
    "    learning_rate = training_schedule[stage]['lr']\n",
    "    for i, (tokens_batch, tags_batch, mask_batch, y_batch) in enumerate(dataset_iterator.gen_batches(batch_size, data_type='train', shuffle=True, binaryTags=False, tag2idx=tag2idx)):\n",
    "        step += 1\n",
    "        losses_train_epoch = []\n",
    "#         print(tokens_batch)\n",
    "#         print(tags_batch)\n",
    "#         print(np.unique(y_batch))\n",
    "        feed = fill_feed_dict({tokens_input_ph: tokens_batch, mask_ph: mask_batch, y_ph: y_batch, learning_rate_ph: learning_rate})\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val1 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val1 = cell1_kernel.eval(session=sess)\n",
    "        # Train\n",
    "        with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "            loss_cur, _ = sess.run([loss, train_op], feed_dict=feed)\n",
    "#             print(np.unique(y_pred_cur))\n",
    "        losses_train_epoch.append(loss_cur)\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            cell0_kernel_val2 = cell0_kernel.eval(session=sess)\n",
    "            cell1_kernel_val2 = cell1_kernel.eval(session=sess)\n",
    "            d_cell0_kernel = np.linalg.norm(cell0_kernel_val2 - cell0_kernel_val1)/np.linalg.norm(cell0_kernel_val1)\n",
    "            d_cell1_kernel = np.linalg.norm(cell1_kernel_val2 - cell1_kernel_val1)/np.linalg.norm(cell1_kernel_val1)\n",
    "            d_elmo_cells_list['cell0'].append(d_cell0_kernel)\n",
    "            d_elmo_cells_list['cell1'].append(d_cell1_kernel)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write('\\r')\n",
    "            progress = i/n_batches_train\n",
    "            sys.stdout.write(\"[%-20s] %d%%\" % ('='*int(ceil(progress*20)), ceil(progress*100)))\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    losses_train.extend(losses_train_epoch)\n",
    "    losses_epoch['train'].append(np.mean(losses_train_epoch))\n",
    "    # Validate\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "#         loss_valid = sess.run([loss], feed_dict=feed_valid)[0]\n",
    "#         if use_crf:\n",
    "#           pred = predict_crf(logits, transition_params, mask_ph, feed_dict=feed_valid)\n",
    "#         else:\n",
    "#           pred = predict_no_crf(y_pred, mask_ph, feed_dict=feed_valid)\n",
    "#     #         print(pred)\n",
    "#         pred = concatenate_arrays(pred)\n",
    "#     #         print(np.unique(pred))\n",
    "#     #         print(np.unique(y_valid_flat))\n",
    "#         f1_valid = f1_score(y_valid_flat, pred, average='macro')\n",
    "        res = eval_valid(dataset_iterator, tag2idx)\n",
    "        f1_valid = res['f1']\n",
    "        loss_valid = res['loss']\n",
    "        if isinstance(f1_valid, list) or isinstance(f1_valid, np.ndarray):\n",
    "            f1_valid = np.mean(f1_valid)\n",
    "        f1_scores['valid'].append(f1_valid)\n",
    "        if f1_valid > best_valid_f1:\n",
    "            best_valid_f1 = f1_valid\n",
    "    # Get elmo params\n",
    "    with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "        if TRAIN_ELMO:\n",
    "            layer_coeff, scale = sess.run([elmo_coef['layer_coefficients'], elmo_coef['scaling']])\n",
    "            if f1_valid == best_valid_f1:\n",
    "                elmo_params_best = {'layer_coefficients': layer_coeff, 'scaling': scale}\n",
    "    losses_epoch['valid'].append(loss_valid)\n",
    "    if epoch % display_epoch == 0 or epoch == 1:\n",
    "        print('Train loss = {}'.format(losses_epoch['train'][-1]))\n",
    "    #         print('Train F1 score = {}'.format(f1_scores['train'][-1]))\n",
    "        if TRAIN_ELMO:\n",
    "            with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "                print('ELMo weights:')\n",
    "                print('Coefficients = {}, scale = {}'.format(layer_coeff, scale))\n",
    "        if TRAIN_ALL_ELMO_PARAMS:\n",
    "            print('ELMo cells change per epoch: cell0: {:.2f}%, cell1: {:.2f}%'.format(d_cell0_kernel*100, d_cell1_kernel*100))\n",
    "\n",
    "    if epoch % valid_epoch == 0 or epoch == 1:\n",
    "        print('Valid loss = {}'.format(losses_epoch['valid'][-1]))\n",
    "        print('Valid F1 score = {}'.format(f1_scores['valid'][-1]))\n",
    "n_steps = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1535747350783,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "DeYhVaYerGPA",
    "outputId": "a3c9c92a-7c1f-4745-852b-865372f7dffe"
   },
   "outputs": [],
   "source": [
    "list(range(1,n_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1535712909777,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "OI5WYrO9VP42",
    "outputId": "637c83b3-a6cb-4df7-84f5-8b7ea1270eca"
   },
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1535712910868,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "V9ovIfbsanNR",
    "outputId": "eae24775-54c0-498c-ca92-f69c0391209f"
   },
   "outputs": [],
   "source": [
    "f1_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1535712912900,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "szSGnmYdPaCA",
    "outputId": "e617cf0e-be52-4528-bc36-f3e420d48e23"
   },
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "plt.plot(epochs, losses_epoch['train'], c='b', label='train')\n",
    "plt.plot(epochs, losses_epoch['valid'], c='r', label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.title('Learning curve')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1535712913958,
     "user": {
      "displayName": "Konstantin Ostrovsky",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109832482076388645622"
     },
     "user_tz": -180
    },
    "id": "JzWORmJ3PaCE",
    "outputId": "961e64ce-5dc4-4977-c189-c3341ca5334e"
   },
   "outputs": [],
   "source": [
    "# Plot F1 scores\n",
    "plt.figure()\n",
    "epochs = np.arange(1, num_epochs+1, 1)\n",
    "# plt.plot(steps, losses['train'], c='b', label='train')\n",
    "plt.plot(epochs, f1_scores['valid'], c='r', label='valid')\n",
    "plt.xlabel('iter')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.title('F1 score history')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2v8VTscyPaCH"
   },
   "outputs": [],
   "source": [
    "elmo_vars_dict = {v.name: v for v in elmo_vars}\n",
    "saver_elmo = tf.train.Saver(elmo_vars_dict)\n",
    "save_path = saver_elmo.save(sess, \"/model_params_multiclass/elmo_multi.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)\n",
    "saver_all = tf.train.Saver()\n",
    "save_path = saver_all.save(sess, \"/model_params_multiclass/model_all_multi.ckpt\")\n",
    "print(\"Whole model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c6697CIGySa"
   },
   "outputs": [],
   "source": [
    "print('Best F1 score on validation: {:.2f}'.format(best_valid_f1*100))\n",
    "if TRAIN_ELMO:\n",
    "    layer_coeff, scale = sess.run([elmo_coef['layer_coefficients'], elmo_coef['scaling']])\n",
    "    elmo_params = {'layer_coefficients': layer_coeff, 'scaling': scale}\n",
    "    elmo_layer_coeff = np.exp(elmo_params['layer_coefficients'])*elmo_params['scaling']\n",
    "    print('Learned ELMo layer combination weights:')\n",
    "    print(elmo_layer_coeff)\n",
    "    print('Normalized:')\n",
    "    print(np.exp(elmo_layer_coeff)/np.sum(np.exp(elmo_layer_coeff)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "17. Finetune ELMo BIO-markup multiclass task.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
